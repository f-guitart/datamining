{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Apache Spark?\n",
    "* distributed framework\n",
    "* in-memory data structures \n",
    "* data processing\n",
    "* it improves (most of the times) Hadoop workloads\n",
    "\n",
    "Spark enables data scientists to tackle problems with larger data sizes than they could before with tools like R or Pandas\n",
    "\n",
    "## First Steps with Apache Spark Interactive Programming\n",
    "\n",
    "First of all check that PySpark is running properly. You can check if PySpark is correctly loaded:\n",
    "In case it is not, you can follow these posts:\n",
    "\n",
    "  * Windows (IPython): http://jmdvinodjmd.blogspot.com.es/2015/08/installing-ipython-notebook-with-apache.html \n",
    "  * Windows (Jupyter): http://www.ithinkcloud.com/tutorials/tutorial-on-how-to-install-apache-spark-on-windows/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"my_spark_app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that with Spark all computation is parallelized by means of distributed data structures that are spread through the cluster. These collections are called Resilient Distributed Datasets (RDD). We will talk more about RDD, as they are the main piece in Spark.\n",
    "\n",
    "As we have successfully loaded the Spark Context, we are ready to do some interactive analysis. We can read a simple file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple first example, where we create an RDD (variable lines) and then we apply some operations (count and first) in a parallel manner. It has to be noted, that as we are running all our examples in a single computer the parallelization is not applied. \n",
    "\n",
    "In the next section we will cover the core Spark concepts that allow Spark users to do parallel computation.\n",
    "\n",
    "## Core Spark Concepts\n",
    "\n",
    "We will talk about **Spark applications** that are in charge of loading data and applying some distributed computation over it. Every application has a **driver program** that launches parallel operations to the cluster. In the case of interactive programming, the driver program is the shell (or Notebook) itself.\n",
    "\n",
    "The \"access point\" to Spark from the driver program is the Spark Context object. \n",
    "\n",
    "Once we have an Spark Context we can use it to build RDDs. In the previous examples we used sc.textFile() to represent the lines of the textFile. Then we run different operations over the RDD lines. \n",
    "\n",
    "To run these operations over RDDs, driver programs manage different nodes called executors. For example, for the count operation, it is possible to run count in different ranges of the file. \n",
    "\n",
    "Spark's API allows passing functions to its operators to run them on the cluster. For example, we could extend our example by filtering the lines in the file that contain a word, such as individuum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "filtered_lines = lines.filter(lambda line: \"individuum\" in line)\n",
    "filtered_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Basics\n",
    "\n",
    "An RDD can be defined as a distributed collection of elements. \n",
    "\n",
    "All work done with Spark can be summarized as **creating**, **transforming** and **applying** operations over RDDs to compute a result. \n",
    "\n",
    "Under the hood, Spark automatically **distributes the data contained in RDDs** across your cluster and **parallelizes the operations** you perform on them.\n",
    "\n",
    "RDD properties:\n",
    "* it is an **immutable distributed** collection of objects\n",
    "* it is split into multiple **partitions**\n",
    "* it is computed on different nodes of the cluster\n",
    "* it can contain any type of Python object (user defined ones included)\n",
    "\n",
    "An RDD can be created in **two ways**:\n",
    "1. loading an external dataset\n",
    "2. distributing a collection of objects in the driver program\n",
    "\n",
    "We have already seen the two ways of creating an RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading an external dataset\n",
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "print(type(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying a transformation to an existing RDD\n",
    "filtered_lines = lines.filter(lambda line: \"individuum\" in line)\n",
    "print(type(filtered_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that once we have an RDD, we can run **two kind of operations**:\n",
    "* **transformations**: construct a new RDD from a previous one. For example, by filtering lines RDD we create a new RDD that holds the lines that contain \"individuum\" string. Note that the returning result is an RDD.\n",
    "* **actions**: *compute* a result based on an RDD, and returns the result to the driver program or stores it to an external storage system (e.g. HDFS). Note that the returning result is not an RDD but another variable type.\n",
    "\n",
    "Notice how when we go to print it, it prints out that it is an RDD and that the type is a PipelinedRDD not a list of values as we might expect. That's because we haven't performed an action yet, we've only performed a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we print lines we get only this\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we perform an action, then we get the result\n",
    "action_result = lines.first()\n",
    "print(type(action_result))\n",
    "action_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations and actions are very different because of the way Spark computes RDDs. \n",
    "\n",
    "Transformations are defined in a **lazy** manner this is they are **only computed once they are used in an action**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_lines is not computed until the next action is applied over it\n",
    "# it make sense when working with big data sets, as it is not necessary to \n",
    "# transform the whole RDD to get an action over a subset\n",
    "# Spark doesn't even reads the complete file!\n",
    "filtered_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback is that Spark  **recomputes** again the RDD at **each action application**. \n",
    "\n",
    "This means that the computing effort over an already computed RDD may be lost. \n",
    "\n",
    "To mitigate this drawback, the user can take the decision of **persisting** the RDD after computing it the first time, **Spark will store the RDD contents in memory**  (partitioned across the machines in your cluster), and reuse them in future actions. \n",
    "\n",
    "**Persisting RDDs on disk** instead of memory is also possible.\n",
    "\n",
    "Let's see an example on the impact of persisting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "lines = sc.textFile(\"../data/REFERENCE/*\")\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "words = lines_nonempty.flatMap(lambda x: x.split())\n",
    "words_persisted = lines_nonempty.flatMap(lambda x: x.split())\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print(\"Word count 1:\",time.time() - t1)\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print(\"Word count 2:\",time.time() - t1)\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.persist()\n",
    "words_persisted.count()\n",
    "print(\"Word count persisted 1:\",time.time() - t1)\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.count()\n",
    "print(\"Word count persisted 2:\", time.time() - t1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "\n",
    "We have already seen that RDDs have two basic operations: **transformations** and **actions**.\n",
    "\n",
    "* **Transformations** are operations that return a new RDD. *Examples:* filter, map.\n",
    "\n",
    "    Remember that , transformed RDDs are **computed lazily**, only when you use them in an action.\n",
    "\n",
    "    Lazy evaluation means that when we call a transformation on an RDD (for instance, calling map()), the operation is **not immediately performed**. \n",
    "\n",
    "    Instead, Spark internally records **metadata** to indicate that this operation has been requested. \n",
    "\n",
    "    **Loading data** into an RDD is lazily evaluated in the same way trans formations are. So, when we call sc.textFile(), the data is **not loaded** until it is necessary. \n",
    "\n",
    "    As with transformations, the operation (in this case, reading the data) can occur multiple times. Take in mind that transformations **DO HAVE** impact over computation time.\n",
    "\n",
    "    Many transformations are **element-wise**; that is, they work on one element at a time; but this is not true for all transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a file\n",
    "lines = sc.textFile(\"../data/REFERENCE/*\")\n",
    "# make a transformation filtering positive length lines\n",
    "lines_nonempty = lines.filter( lambda x: len(x) > 0 )\n",
    "print(\"-> lines_nonepmty is: {} and if we print it we get\\n     {}\".format(type(lines_nonempty), lines_nonempty))\n",
    "\n",
    "# we transform again\n",
    "words = lines_nonempty.flatMap(lambda x: x.split())\n",
    "print(\"-> words is: {} and if we print it we get\\n     {}\".format(type(words), words))\n",
    "\n",
    "words_persisted = lines_nonempty.flatMap(lambda x: x.split())\n",
    "print(\"-> words_persisted is: {} and if we print it we get\\n     {}\".format(type(words_persisted), words_persisted))\n",
    "\n",
    "final_result = words.take(10)\n",
    "print(\"-> final_result is: {} and if we print it we get\\n     {}\".format(type(final_result), final_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **filter** applies the lambda function to each line in lines RDD, only lines that accomplish the condition that the length is greater than zero are in lines_nonempty variable (**this RDD is not computed yet!**)\n",
    "* **flatMap** applies the lambda function to each element of the RDD and then the result is flattened (i.e. a list of lists would be converted to a simple list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Actions** are operations that return an object to the driver program or write to external storage, they kick a computation. *Examples:* first, count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# we checkpint the initial time\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "# and count the time expmended on the computation\n",
    "print(\"Word count 1:\",time.time() - t1)\n",
    "\n",
    "t1 = time.time()\n",
    "words.count()\n",
    "print(\"Word count 2:\",time.time() - t1)\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.persist()\n",
    "words_persisted.count()\n",
    "print(\"Word count persisted 1:\",time.time() - t1)\n",
    "\n",
    "t1 = time.time()\n",
    "words_persisted.count()\n",
    "print(\"Word count persisted 2:\", time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions are the operations that return a **final value** to the driver program or write data to an external storage system. \n",
    "\n",
    "Actions **force the evaluation** of the transformations required for the **RDD** they were called on, since they need to actually produce output.\n",
    "\n",
    "Returning to the previous example, until we call count over words and words persisted, the RDD are not computed. See that we persisted words_persisted, and until its second computation we cannot see the impact of persisting that RDD in memory.\n",
    "\n",
    "If we want to see a part of the RDD, we can use **take**, and to have the full RDD we can use **collect**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "print(\"-> Three elements:\\n\", lines.take(3))\n",
    "print(\"-> The whole RDD:\\n\", lines.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: **Why is not a god idea to collect an RDD?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing functions to Spark\n",
    "\n",
    "Most of Spark’s transformations, and some of its actions, depend on **passing in functions** that are used by Spark to **compute** data.\n",
    "\n",
    "In Python, we have three options for passing functions into Spark. \n",
    " * For shorter functions, we can pass in lambda expressions\n",
    " * We can pass in top-level functions, or \n",
    " * Locally defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"../data/people.csv\")\n",
    "\n",
    "# we create a lambda function to apply tp all lines of the dataset\n",
    "# WARNING, see that after splitting we get only the first element\n",
    "first_cells = lines.map(lambda x: x.split(\",\")[0])\n",
    "print(first_cells.collect())\n",
    "\n",
    "# we can define a function as well\n",
    "def get_cell(x):\n",
    "    return x.split(\",\")[0]\n",
    "first_cells = lines.map(get_cell)\n",
    "print(first_cells.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with common Spark transformations\n",
    "\n",
    "The two most common transformations you will likely be using are **map** and **filter**. \n",
    "\n",
    "The **map()** transformation takes in a function and applies it to each element in the RDD with the result of the function being the new value of each element in the resulting RDD. \n",
    "\n",
    "The **filter()** transformation takes in a function and returns an RDD that only has elements that pass the filter() function.\n",
    "\n",
    "Sometimes **map()** returns nested lists, to flatten these nested lists we can use **flatMap()**. \n",
    "\n",
    "So, **flatMap()** is called individually for each element in our input RDD. Instead of returning a single element, we return an iterator with our return values. Rather than producing an RDD of iterators, we get back an RDD that consists of the elements from all of the iterators.\n",
    "\n",
    "### Set operations\n",
    "\n",
    "* **distinct()** transformation to produce a new RDD with only distinct items. \n",
    "    \n",
    "    **Note that distinct() is expensive**, however, as it requires shuffling all the data over the network to ensure that we receive only one copy of each element\n",
    "    \n",
    "\n",
    "* **RDD.union(other)** back an RDD consisting of the data from both sources. \n",
    "\n",
    "    Unlike the mathematical union(), if there are duplicates in the input RDDs, the result of Spark’s union() will contain duplicates (which we can fix if desired with distinct()).\n",
    "\n",
    "\n",
    "* **RDD.intersection(other)**  returns only elements in both RDDs. intersection() also removes all duplicates (including duplicates from a single RDD) while running. \n",
    "\n",
    "    While intersection() and union() are two similar concepts, the performance of intersection() is much worse since it requires a shuffle over the network to identify common elements.\n",
    "\n",
    "\n",
    "* **RDD.subtract(other)** function takes in another RDD and returns an RDD that has only values present in the first RDD and not the second RDD. Like intersection(), it performs a shuffle.\n",
    "\n",
    "\n",
    "* **RDD.cartesian(other)** transformation returns all possible pairs of (a,b) where a is in the source RDD and b is in the other RDD. \n",
    "\n",
    "    The Cartesian product can be useful when we wish to consider the similarity between all possible pairs, such as computing every user’s expected interest in each offer. We can also take the Cartesian product of an RDD with itself, which can be useful for tasks like user similarity. Be warned, however, that the Cartesian product is very expensive for large RDDs.\n",
    "\n",
    "### Actions\n",
    "\n",
    "* **reduce():** which takes a function that operates on two elements of the type in your RDD and returns a new element of the same type. \n",
    "\n",
    "\n",
    "* **aggregate():** takes an initial zero value of the type we want to return. We then supply a function to combine the elements from our RDD with the accumulator. Finally, we need to supply a second function to merge two accumulators, given that each node accumulates its own results locally. To know more:\n",
    "    * http://stackoverflow.com/questions/28240706/explain-the-aggregate-functionality-in-spark\n",
    "    * http://atlantageek.com/2015/05/30/python-aggregate-rdd/\n",
    "    \n",
    "    \n",
    "* **collect():** returns the entire RDD’s contents. collect() is commonly used in unit tests where the entire contents of the RDD are expected to fit in memory, as that makes it easy to compare the value of our RDD with our expected result.\n",
    "\n",
    "\n",
    "* **take(n):** returns n elements from the RDD and attempts to minimize the number of partitions it accesses, so it may represent a biased collection\n",
    "\n",
    "\n",
    "* **top():** will use the default ordering on the data, but we can supply our own comparison function to extract the top elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "We have a file (../data/books.csv) with a lot of links to books. We want to perform an analysis to the books and its contents.\n",
    "\n",
    "**Exercise 1:** Download all books, from books.csv using the map function.\n",
    "\n",
    "**Exercise 2:** Identify transformations and actions. When the returned data is calculated?\n",
    "\n",
    "**Exercise 3:** Imagine that you only want to download Dickens books, how would you do that? Which is the impact of not persisting dickens_books_content?\n",
    "\n",
    "**Exercise 4:** Use flatMap() in the resulting RDD of the previous exercise, how the result is different?\n",
    "\n",
    "**Exercise 5:** You want to know the different books authors there are.\n",
    "\n",
    "**Exercise 6:** Return Poe's and Dickens' books URLs (use union function).\n",
    "\n",
    "**Exercise 7:** Return the list of books without Dickens' and Poe's books.\n",
    "\n",
    "**Exercise 8:** Count the number of books using reduce function.\n",
    "\n",
    "For the following two exercices, we will use ../data/Sacramentorealestatetransactions.csv\n",
    "\n",
    "**Exercise 9:** Compute the mean price of estates from csv containing Sacramento's estate price using aggregate function.\n",
    "\n",
    "**Exercise 10:** Get top 5 highest and lowest prices in Sacramento estate's transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Download all books, from books.csv using the map function.\n",
    "\n",
    "**Answer 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "def download_file(csv_line):\n",
    "    link = csv_line[0]\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request('GET', link, preload_content=False)\n",
    "    response = r.read()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_info = sc.textFile(\"../data/books.csv\").map(lambda x: x.split(\",\"))\n",
    "print(books_info.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_content = books_info.map(download_file)\n",
    "print(books_content.take(10)[1][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Identify transformations and actions. When the returned data is calculated?\n",
    "\n",
    "**Answer 2:**\n",
    "If we consider the text reading as a transformation...\n",
    "\n",
    "*Transformations:*\n",
    "* books_info = sc.textFile(\"../data/books.csv\").map(lambda x: x.split(\",\"))\n",
    "* books_content = books_info.map(lambda x: download_file(x[0]))\n",
    "\n",
    "*Actions:*\n",
    "* print books_info.take(10)\n",
    "* print books_content.take(1)[0][:100]\n",
    "\n",
    "Computation is carried out in actions. In this case we take advantage of it, as for downloading data we only apply the function to one element of the books_content RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Imagine that you only want to download Dickens books, how would you do that? Which is the impact of not persisting dickens_books_content?\n",
    "\n",
    "**Answer 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_dickens(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/DICKENS/\",link)\n",
    "    return t != None\n",
    "\n",
    "dickens_books_info = books_info.filter(is_dickens)\n",
    "print(dickens_books_info.take(4))\n",
    "\n",
    "dickens_books_content = dickens_books_info.map(download_file)\n",
    "\n",
    "# take into consideration that each time an action is performed over dickens_book_content, the file is downloaded\n",
    "# this has a big impact into calculations\n",
    "print(dickens_books_content.take(2)[1][:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Use flatMap() in the resulting RDD of the previous exercise, how the result is different?\n",
    "\n",
    "**Answer 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flat_content = dickens_books_info.map(lambda x: x)\n",
    "print(flat_content.take(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_content = dickens_books_info.flatMap(lambda x: x)\n",
    "print(flat_content.take(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** You want to know the different books authors there are.\n",
    "\n",
    "**Answer 5:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/(\\w+)/\",link)\n",
    "    if t:\n",
    "        return t.group(1)\n",
    "    return u'UNKNOWN'\n",
    "\n",
    "authors = books_info.map(get_author)\n",
    "authors.distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise 6:** Return Poe's and Dickens' books URLs (use union function).\n",
    "\n",
    "**Answer 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_author_and_link(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/(\\w+)/\",link)\n",
    "    if t:\n",
    "        return (t.group(1), link)\n",
    "    return (u'UNKNOWN',link)\n",
    "\n",
    "authors_links = books_info.map(get_author_and_link)\n",
    "\n",
    "# not very efficient\n",
    "dickens_books = authors_links.filter(lambda x: x[0]==\"DICKENS\")\n",
    "poes_books = authors_links.filter(lambda x: x[0]==\"POE\")\n",
    "\n",
    "poes_dickens_books = poes_books.union(dickens_books)\n",
    "\n",
    "# sample is a transformation that returns an RDD sampled over the original RDD\n",
    "# https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html\n",
    "poes_dickens_books.sample(True,0.05).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeSample is an action, returning a sampled subset of the RDD\n",
    "poes_dickens_books.takeSample(True,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Return the list of books without Dickens' and Poe's books.\n",
    "\n",
    "**Answer 7:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_links.subtract(poes_dickens_books).map(lambda x: x[0]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** Count the number of books using reduce function.\n",
    "\n",
    "**Answer 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_links.map(lambda x: 1).reduce(lambda x,y: x+y) == authors_links.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see this approach more in detail\n",
    "\n",
    "# this transformation generates an rdd of 1, one per element in the RDD\n",
    "authors_map = authors_links.map(lambda x: 1)\n",
    "authors_map.takeSample(True,10)\n",
    "\n",
    "# with reduce, we pass a function with two parameters which is applied by pairs\n",
    "# inside the the function we specify which operation we perform with the two parameters\n",
    "# the result is then returned and the action is applied again using the result until there is only one element in the resulting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a very efficient way to do a summation in parallel\n",
    "# using a functional approach\n",
    "# we could define any operation inside the function\n",
    "authors_map.reduce(lambda x,y: x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:** Compute the mean price of estates from csv containing Sacramento's estate price using aggregate function.\n",
    "\n",
    "**Answer 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sacramento_estate_csv = sc.textFile(\"../data/Sacramentorealestatetransactions.csv\")\n",
    "header = sacramento_estate_csv.first()\n",
    "\n",
    "# first load the data\n",
    "# we know that the price is in column 9\n",
    "sacramento_estate = sacramento_estate_csv.filter(lambda x: x != header)\\\n",
    "        .map(lambda x: x.split(\",\"))\\\n",
    "        .map(lambda x: int(x[9]))\n",
    "\n",
    "sacramento_estate.takeSample(True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqOp = (lambda x,y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "total_sum, number = sacramento_estate.aggregate((0,0),seqOp,combOp)\n",
    "mean = float(total_sum)/number\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10:** Get top 5 highest and lowest prices in Sacramento estate's transactions\n",
    "\n",
    "**Answer 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sacramento_estate.top(5))\n",
    "print(sacramento_estate.top(5, key=lambda x: -x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spark Key/Value Pairs\n",
    "\n",
    "Spark provides special operations on RDDs containing key/value pairs. \n",
    "\n",
    "These RDDs are called pair RDDs, but are simple RDDs with an special structure. In Python, for the functions on keyed data to work we need to return an RDD composed of tuples.\n",
    "\n",
    "**Exercise 1:** Create a pair RDD from our books information data, having author as key and the rest of the information as value. (Hint: the answer is very similar to the previous section Exercise 6)\n",
    "\n",
    "**Exercise 2:** Check that pair RDDs are also RDDs and that common RDD operations work as well. Filter elements with author equals to \"UNKNOWN\" from previous RDD. \n",
    "\n",
    "**Exercise 3:** Check mapValue in Spark API (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapValues) function that works on pair RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Create a pair RDD from our books information data, having author as key and the rest of the information as value. (Hint: the answer is very similar to the previous section Exercise 6)\n",
    "\n",
    "**Answer 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_author_data(csv_line):\n",
    "    link = csv_line[0]\n",
    "    t = re.match(\"http://www.textfiles.com/etext/AUTHORS/(\\w+)/\",link)\n",
    "    if t:\n",
    "        return (t.group(1), csv_line)\n",
    "    return (u'UNKNOWN', csv_line)\n",
    "\n",
    "books_info = sc.textFile(\"../data/books.csv\").map(lambda x: x.split(\",\"))\n",
    "authors_info = books_info.map(get_author_data)\n",
    "\n",
    "print(authors_info.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise 2:** Check that pair RDDs are also RDDs and that common RDD operations work as well. Filter elements with author equals to \"UNKNOWN\" from previous RDD.\n",
    "\n",
    "**Answer 2:**\n",
    "\n",
    "The operations over pair RDDs will also be slightly different.\n",
    "\n",
    "But take into account that pair RDDs are just *special* RDDs that some operations can be applied, however common RDDs also fork for them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_info.filter(lambda x: x[0] != \"UNKNOWN\").take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Check mapValue in Spark API (http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapValues) function that works on pair RDDs.\n",
    "\n",
    "**Answer 3:** \n",
    "\n",
    "Sometimes is awkward to work with pairs, and Spark provides a map function that operates over values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_info.mapValues(lambda x: x[2]).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on Pair RDDs\n",
    "\n",
    " Since pair RDDs contain tuples, we need to pass functions that operate on tuples rather than on individual elements.\n",
    " \n",
    " * *reduceByKey(func):* Combine values with the same key.\n",
    " * *groupByKey():* Group values with the same key.\n",
    " * *combineByKey(createCombiner, mergeValue, mergeCombiners, partitioner):* Combine values with the same key using a different result type.\n",
    " * *keys():* return RDD keys\n",
    " * *values():* return RDD values\n",
    " * *groupBy():* takes a function that it applies to every element in the source RDD and uses the result to determine the key.\n",
    " * *cogroup():* over two RDDs sharing the same key type, K, with the respective value types V and W gives us back RDD[(K,(Iterable[V], Iterable[W]))]. If one of the RDDs doesn’t have elements for a given key that is present in the other RDD, the corresponding Iterable is simply empty. cogroup() gives us the power to group data from multiple RDDs.\n",
    "\n",
    "**Exercise 1:** Get the total size of files for each author. \n",
    "\n",
    "**Exercise 2:** Get the top 5 authors with more data.\n",
    "\n",
    "**Exercise 3:** Try the combineByKey() with a randomly generated set of 5 values for 4 keys. Get the average value of the random variable for each key.\n",
    "\n",
    "**Exercise 4:** Compute the average book size per author using combineByKey(). If you were an English Literature student and your teacher says: \"Pick one Author and I'll randomly pick a book for you to read\", what would be a Data Scientist answer?\n",
    "\n",
    "**Exercise 5:** All Spark books have the word count example. Let's count words over all our books! (This might take some time)\n",
    "\n",
    "**Exercise 6:** Group author data by author surname initial. How many authors have we grouped? \n",
    "\n",
    "**Exercise 7:** Generate a pair RDD with alphabet letters in upper case as key, and empty list as value. Then group the previous RDD with this new one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Get the total size of files for each author. \n",
    "\n",
    "**Answer 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get each book size, keyed by author\n",
    "authors_data = authors_info.mapValues(lambda x: int(x[2]))\n",
    "authors_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ther reduce summing\n",
    "authors_data.reduceByKey(lambda y,x: y+x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Get the top 5 authors with more data.\n",
    "\n",
    "**Answer 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_data.reduceByKey(lambda y,x: y+x).top(5,key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Try the combineByKey() with a randomly generated set of 5 values for 4 keys. Get the average value of the random variable for each key.\n",
    "\n",
    "**Answer 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate the data\n",
    "for pair in list(zip(np.arange(5).tolist()*5, np.random.normal(0,1,5*5))):\n",
    "    print(pair)\n",
    "    \n",
    "rdd = sc.parallelize(zip(np.arange(5).tolist()*5, np.random.normal(0,1,5*5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createCombiner = lambda value: (value,1)\n",
    "# you can check what createCombiner does\n",
    "# rdd.mapValues(createCombiner).collect()\n",
    "\n",
    "# here x is the combiner (sum,count) and value is value in the \n",
    "# initial RDD (the random variable)\n",
    "mergeValue = lambda x, value: (x[0] + value, x[1] + 1)\n",
    "\n",
    "# here, all combiners are summed (sum,count)\n",
    "mergeCombiner = lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "\n",
    "sumCount = rdd.combineByKey(createCombiner,\n",
    "                        mergeValue,\n",
    "                         mergeCombiner)\n",
    "\n",
    "print(sumCount.collect())\n",
    "sumCount.mapValues(lambda x: x[0]/x[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Compute the average book size per author using combineByKey(). If you were an English Literature student and your teacher says: \"Pick one Author and I'll randomly pick a book for you to read\", what would be a Data Scientist answer?\n",
    "\n",
    "**Answer 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "createCombiner = lambda value: (value,1)\n",
    "# you can check what createCombiner does\n",
    "# rdd.mapValues(createCombiner).collect()\n",
    "\n",
    "# here x is the combiner (sum,count) and value is value in the \n",
    "# initial RDD (the random variable)\n",
    "mergeValue = lambda x, value: (x[0] + value, x[1] + 1)\n",
    "\n",
    "# here, all combiners are summed (sum,count)\n",
    "mergeCombiner = lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "\n",
    "sumCount = authors_data.combineByKey(createCombiner,\n",
    "                        mergeValue,\n",
    "                         mergeCombiner)\n",
    "\n",
    "print(sumCount.mapValues(lambda x: x[0]/x[1]).collect())\n",
    "# I would choose the author with lowest average book size\n",
    "print(sumCount.mapValues(lambda x: x[0]/x[1]).top(5,lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** All Spark books have the word count example. Let's count words over all our books! (This might take some time)\n",
    "\n",
    "**Answer 5:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import re\n",
    "\n",
    "def download_file(csv_line):\n",
    "    link = csv_line[0]\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request('GET', link, preload_content=False)\n",
    "    response = r.read()\n",
    "    return str(response)\n",
    "    \n",
    "books_info = sc.textFile(\"../data/books.csv\").map(lambda x: x.split(\",\"))\n",
    "#books_content = books_info.map(download_file)\n",
    "# while trying the function use only two samples\n",
    "books_content = sc.parallelize(books_info.map(download_file).take(2))\n",
    "\n",
    "words_rdd = books_content.flatMap(lambda x: x.split(\" \")).\\\n",
    "                          flatMap(lambda x: x.split(\"\\r\\n\")).\\\n",
    "                          map(lambda x: re.sub('[^0-9a-zA-Z]+', '', x).lower()).\\\n",
    "                          filter(lambda x: x != '')\n",
    "\n",
    "words_rdd.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).top(5, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Group author data by author surname initial. How many authors have we grouped? \n",
    "\n",
    "**Answer 6:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(authors_info.groupBy(lambda x: x[0][0]).collect())\n",
    "\n",
    "authors_info.map(lambda x: x[0]).distinct().\\\n",
    "                map(lambda x: (x[0],1)).\\\n",
    "                reduceByKey(lambda x,y: x+y).\\\n",
    "                filter(lambda x: x[1]>1).\\\n",
    "                collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Generate a pair RDD with alphabet letters in upper case as key, and empty list as value. Then group the previous RDD with this new one.\n",
    "\n",
    "**Answer 7:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "sc.parallelize(list(string.ascii_uppercase)).\\\n",
    "                                map(lambda x: (x,[])).\\\n",
    "                                cogroup(authors_info.groupBy(lambda x: x[0][0])).\\\n",
    "                                take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "Some of the most useful operations we get with keyed data comes from using it together with other keyed data. \n",
    "\n",
    "Joining data together is probably one of the most common operations on a pair RDD, and we have a full range of options including right and left outer joins, cross joins, and inner joins.\n",
    "\n",
    "### Inner Join\n",
    "\n",
    "Only keys that are present in both pair RDDs are output. \n",
    "\n",
    "When there are multiple values for the same key in one of the inputs, the resulting pair RDD will have an entry for every possible pair of values with that key from the two input RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Take countries_data_clean.csv and countries_GDP_clean.csv and join them using country name as key. \n",
    "\n",
    "Before doing the join, please, check how many element should the resulting pair RDD have. \n",
    "\n",
    "After the join, check if the initial hypothesis was true. \n",
    "\n",
    "In case it is not, what is the reason? \n",
    "\n",
    "How would you resolve that problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more info: https://www.worlddata.info/downloads/\n",
    "rdd_countries = sc.textFile(\"../data/countries_data_clean.csv\").map(lambda x: x.split(\",\"))\n",
    "#more info: http://data.worldbank.org/data-catalog/GDP-ranking-table\n",
    "rdd_gdp = sc.textFile(\"../data/countries_GDP_clean.csv\").map(lambda x: x.split(\";\"))\n",
    "\n",
    "# check rdds size\n",
    "hyp_final_rdd_num = rdd_gdp.count() if rdd_countries.count() > rdd_gdp.count() else rdd_countries.count()\n",
    "print(\"The final number of elements in the joined rdd should be: \", hyp_final_rdd_num)\n",
    "p_rdd_gdp = rdd_gdp.map(lambda x: (x[3],x))\n",
    "p_rdd_countries = rdd_countries.map(lambda x: (x[1],x))\n",
    "\n",
    "print(p_rdd_countries.take(1))\n",
    "print(p_rdd_gdp.take(1))\n",
    "\n",
    "p_rdd_contry_data = p_rdd_countries.join(p_rdd_gdp)\n",
    "\n",
    "final_join_rdd_size = p_rdd_contry_data.count()\n",
    "hyp = hyp_final_rdd_num == final_join_rdd_size\n",
    "print(\"The initial hypothesis is \", hyp)\n",
    "if not hyp:\n",
    "    print(\"The final joined rdd size is \", final_join_rdd_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left and Right outer Joins\n",
    "\n",
    "Sometimes we don’t need the key to be present in both RDDs to want it in our result.\n",
    "\n",
    "For example, imagine that our list of countries is not complete, and we don't want to miss data if it a country is not present in both RDDs.\n",
    "\n",
    "**leftOuterJoin(other)** and **rightOuterJoin(other)** both join pair RDDs together by key, where one of the pair RDDs can be missing the key.\n",
    "\n",
    "With **leftOuterJoin()** the resulting pair RDD has entries for each key in the source RDD.\n",
    "\n",
    "The value associated with each key in the result is a tuple of the value from the source RDD and an Option for the value from the other pair RDD. \n",
    "\n",
    "In Python, if a value isn’t present None is used; and if the value is present the regular value, without any wrapper, is used. \n",
    "\n",
    "As with join(), we can have multiple entries for each key; when this occurs, we get the Cartesian product between the two lists of values.\n",
    "\n",
    "**rightOuterJoin()** is almost identical to leftOuterJoin() except the key must be present in the other RDD and the tuple has an option for the source rather than the other RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Use two simple RDDs to show the results of left and right outer join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "rdd_1 = sc.parallelize([(x,1) for x in range(n)])\n",
    "rdd_2 = sc.parallelize([(x*2,1) for x in range(n)])\n",
    "\n",
    "print(\"rdd_1: \",rdd_1.collect())\n",
    "print(\"rdd_2: \",rdd_2.collect())\n",
    "\n",
    "print(\"leftOuterJoin: \",rdd_1.leftOuterJoin(rdd_2).collect())\n",
    "print(\"rightOuterJoin: \",rdd_1.rightOuterJoin(rdd_2).collect())\n",
    "\n",
    "print(\"join: \", rdd_1.join(rdd_2).collect())\n",
    "\n",
    "#explore what hapens if a key is present twice or more\n",
    "rdd_3 = sc.parallelize([(x*2,1) for x in range(n)] + [(4,2),(6,4)])\n",
    "print(\"rdd_3: \",rdd_3.collect())\n",
    "print(\"join: \", rdd_2.join(rdd_3).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Generate two pair RDDs with country info:\n",
    "1. A first one with country code and GDP\n",
    "2. A second one with country code and life expectancy\n",
    "\n",
    "Then join them to have a pair RDD with country code plus GDP and life expentancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Inspect the dataset with GDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_gdp = sc.textFile(\"../data/countries_GDP_clean.csv\").map(lambda x: x.split(\";\"))\n",
    "rdd_gdp.take(2)\n",
    "#generate a pair rdd with countrycode and GDP\n",
    "rdd_cc_gdp = rdd_gdp.map(lambda x: (x[1],x[4]))\n",
    "rdd_cc_gdp.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the dataset with life expectancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_countries = sc.textFile(\"../data/countries_data_clean.csv\").map(lambda x: x.split(\",\"))\n",
    "print(rdd_countries.take(2))\n",
    "#generate a pair rdd with countrycode and lifexpectancy \n",
    "#(more info in https://www.worlddata.info/downloads/)\n",
    "#we don't have countrycode in this dataset, but let's try to add it\n",
    "#we have a dataset with countrynames and countrycodes\n",
    "#let's take countryname and ISO 3166-1 alpha3 code\n",
    "rdd_cc = sc.textFile(\"../data/countrycodes.csv\").\\\n",
    "                    map(lambda x: x.split(\";\")).\\\n",
    "                    map(lambda x: (x[0].strip(\"\\\"\"),x[4].strip(\"\\\"\"))).\\\n",
    "                    filter(lambda x: x[0] != 'Country (en)')\n",
    "print(rdd_cc.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_cc_info = rdd_countries.map(lambda x: (x[1],x[16]))\n",
    "rdd_cc_info.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's count and see if something is missing\n",
    "print(rdd_cc.count())\n",
    "print(rdd_cc_info.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take only values, the name is no longer needed\n",
    "rdd_name_cc_le = rdd_cc_info.leftOuterJoin(rdd_cc)\n",
    "rdd_cc_le = rdd_name_cc_le.map(lambda x: x[1])\n",
    "print(rdd_cc_le.take(5))\n",
    "print(rdd_cc_le.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is missing?\n",
    "rdd_name_cc_le.filter(lambda x: x[1][1] == None).collect()\n",
    "#how can we solve this problem??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some missing data, that we have to complete, but we have quite a lot of data, let's follow.\n",
    "\n",
    "Inspect the results of GDP and life expectancy and join them. **Is there some data missing?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is there some data missing?\", rdd_cc_gdp.count() != rdd_cc_le.count())\n",
    "print(\"GDP dataset: \", rdd_cc_gdp.count())\n",
    "print(\"Life expectancy dataset: \", rdd_cc_le.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try to see what happens\n",
    "print(rdd_cc_le.take(10))\n",
    "print (rdd_cc_gdp.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_cc_gdp_le = rdd_cc_le.map(lambda x: (x[1],x[0])).leftOuterJoin(rdd_cc_gdp)\n",
    "#we have some countries that the data is missing\n",
    "# we have to check if this data is available\n",
    "# or there is any error\n",
    "rdd_cc_gdp_le.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Data\n",
    "\n",
    "sortByKey(): We can sort an RDD with key/value pairs provided that there is an ordering defined on the key.\n",
    "\n",
    "Once we have sorted our data, any subsequent call on the sorted data to collect() or save() will result in ordered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Sort country data by key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_rdd_contry_data.sortByKey().take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions over Pair RDDs\n",
    "\n",
    "* countByKey(): Count the number of elements for each key.\n",
    "* collectAsMap(): Collect the result as a map to provide easy lookup.\n",
    "* lookup(key): Return all values associated with the provided key.\n",
    "\n",
    "**Exercises:**\n",
    "    1. Count countries RDD by key\n",
    "    2. Collect countries RDD as map\n",
    "    3. Lookup Andorra info in countries RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_rdd_contry_data.countByKey()[\"Andorra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_rdd_contry_data.collectAsMap()[\"Andorra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_rdd_contry_data.lookup(\"Andorra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning\n",
    "\n",
    "(*from: Learning Spark - O'Reilly*)\n",
    "\n",
    "Spark programs can choose to control their RDDs’ partitioning to reduce communication. \n",
    "\n",
    "Partitioning will not be helpful in all applications— for example, if a given RDD is scanned only once, there is no point in partitioning it in advance. \n",
    "\n",
    "It is useful only when a dataset is reused multiple times in key-oriented operations such as joins.\n",
    "\n",
    "Spark’s partitioning is **available on all RDDs of key/value pairs**, and causes the system to **group elements based on a function of each key**.\n",
    "\n",
    "Spark does not give explicit control of which worker node each key goes to (partly because the system is designed to work even if specific nodes fail), it lets the program ensure that a set of keys will appear together on some node. \n",
    "\n",
    "**Example:**\n",
    "\n",
    "As a simple example, consider an application that keeps a large table of user information in memory—say, an RDD of (UserID, UserInfo) pairs, where UserInfo contains a list of topics the user is subscribed to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_userinfo = sc.textFile(\"../data/users_events_example/user_info_1000users_20topics.csv\")\\\n",
    "                    .filter(lambda x: len(x)>0)\\\n",
    "                    .map(lambda x: (x.split(\",\")[0],x.split(\",\")[1].split(\"|\")))\n",
    "rdd_userinfo.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application periodically combines this table with a smaller file representing events that happened in the past five minutes—say, a table of (UserID, LinkInfo) pairs for users who have clicked a link on a website in those five minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_userevents = sc.textFile(\"../data/users_events_example/userevents_*.log\")\\\n",
    "                                .filter(lambda x: len(x))\\\n",
    "                                .map(lambda x: (x.split(\",\")[1], [x.split(\",\")[2]]))\n",
    "print(rdd_userevents.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we may wish to count how many users visited a link that was not to one of their subscribed topics. We can perform this combination with Spark’s join() operation, which can be used to group the User Info and LinkInfo pairs for each UserID by key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_joined = rdd_userinfo.join(rdd_userevents)\n",
    "print(rdd_joined.count())\n",
    "print(rdd_joined.filter(lambda x: (x[1][1][0] not in x[1][0])).count())\n",
    "print(rdd_joined.filter(lambda x: (x[1][1][0] in x[1][0])).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we want to count the number of visits to non-subscribed visits using a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_userinfo = sc.textFile(\"../data/users_events_example/user_info_1000users_20topics.csv\")\\\n",
    "                    .filter(lambda x: len(x)>0)\\\n",
    "                    .map(lambda x: (x.split(\",\")[0],x.split(\",\")[1].split(\"|\"))).persist()\n",
    "        \n",
    "def process_new_logs(event_fite_path):\n",
    "    rdd_userevents = sc.textFile(event_fite_path)\\\n",
    "                                .filter(lambda x: len(x))\\\n",
    "                                .map(lambda x: (x.split(\",\")[1], [x.split(\",\")[2]]))\n",
    "    rdd_joined = rdd_userinfo.join(rdd_userevents)\n",
    "    print(\"Number of visits to non-subscribed topics: \",\n",
    "        rdd_joined.filter(lambda x: (x[1][1][0] not in x[1][0])).count())\n",
    "        \n",
    "process_new_logs(\"../data/users_events_example/userevents_01012016000500.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will run fine as is, but it will be inefficient. \n",
    "\n",
    "This is because the join() operation, called each time process_new_logs() is invoked, does not know anything about how the keys are partitioned in the datasets. \n",
    "\n",
    "By default, this operation will hash all  the keys of both datasets, sending elements with the same key hash across the network to the same machine, and then join together the elements with the same key on that machine (see figure below). \n",
    "\n",
    "![user partitioning example rdds](https://github.com/f-guitart/data_mining/blob/master/notes/img/user_partitioning_example_rdds.png?raw=true \"user partitioning example rdds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we expect the rdd_userinfo table to be much larger than the small log of events seen every five minutes, this wastes a lot of work: the rdd_userinfo table is hashed and shuffled across the network on every call, even though it doesn’t change.\n",
    "\n",
    "Fixing this is simple: just use the **partitionBy()** transformation on rdd_userinfo to hash-partition it at the start of the program. We do this by passing a spark.HashPartitioner object to partitionBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_userinfo = sc.textFile(\"../data/users_events_example/user_info_1000users_20topics.csv\")\\\n",
    "                    .filter(lambda x: len(x)>0)\\\n",
    "                    .map(lambda x: (x.split(\",\")[0],x.split(\",\")[1].split(\"|\"))).partitionBy(10)    \n",
    "rdd_userinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process_new_logs() method can remain unchanged: the rdd_userevents RDD is local to process_new_logs(), and is used only once within this method, so there is no advantage in specifying a partitioner for events. \n",
    "\n",
    "Because we called partitionBy() when building userData, Spark will now know that it is hash-partitioned, and calls to join() on it will take advantage of this information. \n",
    "\n",
    "In particular, when we call user rdd_userinfo.join(rdd_userevents), Spark will shuffle only the events RDD, sending events with each particular UserID to the machine that contains the corresponding hash partition of rdd_userinfo. \n",
    "\n",
    "The result is that a lot less data is communicated over the network, and the program runs significantly faster.\n",
    "\n",
    "![user partitioning example rdds 2](https://github.com/f-guitart/data_mining/blob/master/notes/img/user_partitioning_example_rdds_partitionBy.png?raw=true \"user partitioning example rdds 2\")\n",
    "\n",
    "More on partitioning:\n",
    " * http://stackoverflow.com/questions/35973590/pyspark-partioning-data-using-partitionby\n",
    " * http://stackoverflow.com/questions/31424396/apache-spark-hashpartitioner-how-does-it-work\n",
    "\n",
    "Note that partitionBy() is a transformation, so it always returns a new RDD—it does not change the original RDD in place. RDDs can never be modified once created. Therefore it is important to persist and save as rdd_userinfo the result of partitionBy(), not the original textFile().\n",
    "\n",
    "Also, the 100 passed to partitionBy() represents the number of partitions, which will control how many parallel tasks perform further operations on the RDD (e.g., joins); in general, **make this at least as large as the number of cores in your cluster**.\n",
    "\n",
    "In fact, many other Spark operations automatically result in an RDD with known partitioning information, and many operations other than join() will take advantage of this information.\n",
    "\n",
    "For example, sortByKey() and groupByKey() will result in range-partitioned and hash-partitioned RDDs, respectively. \n",
    "\n",
    "On the other hand, operations like map() cause the new RDD to forget the parent’s partitioning information, because such operations could theoretically modify the key of each record. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
