{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Technically Correct to Consistent Data\n",
    "\n",
    "(*from de Jonge van der Loo*)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Consistent data is technically correct data that is ready for statistical analysis.**\n",
    "\n",
    "This implies that data with missing values, special values, errors and outliers are, either:\n",
    "* removed,\n",
    "* corrected, or\n",
    "* imputed\n",
    "\n",
    "The data is consistent with constraints based on real-world knowledge about the subject that the data describe.\n",
    "\n",
    "Consistency can be understood to include:\n",
    "* **in-record consistency:** no contradictory information is stored in a single record\n",
    "* **cross-record consistency:** meaning that statistical summaries of different variables do not conflict with each other\n",
    "* **cross-dataset consistency:** meaning that the dataset that is currently analyzed is consistent with other datasets pertaining to the same subject matter\n",
    "\n",
    "We mainly will focus on methods dealing with **in-record consistency**, with the exception of outlier handling which can be considered a cross-record consistency issue.\n",
    "\n",
    "The process towards consistent data always involves the following 3 steps:\n",
    "\n",
    "1. **Detection** of an inconsistency: find which constraints are violated. For example, an age variable is constrained to non-negative values.\n",
    "2. **Selection** of the field or fields causing the inconsistency. This is trivial in the case of a univariate demand as in the previous step, but may be more cumbersome when cross-variable relations are expected to hold. For example the marital status of a child must be unmarried. In the case of a violation it is not immediately clear whether age, marital status or both are wrong.\n",
    "3. **Correction** of the fields that are deemed erroneous by the selection method. This may be done through deterministic (model-based) or stochastic methods\n",
    "\n",
    "The execution of three steps won't be always neated divided in different code executions. They can be used all in one execution. However, it is important to keep in mind the process towards Consistent Data.\n",
    "\n",
    "### What you will learn in this session\n",
    "* How to use `pandas` library to detect and localize errors\n",
    "* Check how these errors relate with the consistency of the dataset\n",
    "* How can we deal with these errors and correct them\n",
    "* How to impute missing values\n",
    "\n",
    "## Contents\n",
    "* [Detection and Localization of Errors](#Detection-and-Localization-of-Errors)\n",
    "    * [Missing Values](#Missing-Values)\n",
    "    * [Special Values](#Special-Values)\n",
    "    * [Error Localization](#Error-Localization)\n",
    "    * [Outliers](#Outliers)\n",
    "* [Error Correction](#Error-Correction)\n",
    "    * [Simple transformation](#Simple-transformation)\n",
    "* [Imputation](#Imputation)\n",
    "    * [`DataFrame.fillna`]([#DataFrame.fillna])\n",
    "    * [`Series.interpolate`](#Series.interpolate)\n",
    "    * [`DataFrame.dropna`](#DataFrame.dropna)\n",
    "    * [Basic Imputation Methods](#Basic-Imputation-Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection and Localization of Errors\n",
    "\n",
    "This is the first step towards consistency satisfaction: to detect and localize all possible errors.\n",
    "\n",
    "We will put errors into three groups:\n",
    "* Missing Values\n",
    "* Special Values\n",
    "* Outliers\n",
    "\n",
    "### Missing Values\n",
    "\n",
    "A missing value, represented by (`np.nan` in Pandas), is a placeholder for a datum of which **the type is known but its value isn't** (technically correct but not consistent). \n",
    "\n",
    "Therefore, **it is impossible to perform statistical analysis on data where one or\n",
    "more values in the data are missing**.\n",
    "\n",
    "We have two options:\n",
    "1. omit elements from a dataset that contain missing values\n",
    "2. impute a value\n",
    "\n",
    "**Missingness is something to be dealt with prior to any analysis.**\n",
    "\n",
    "In Pandas however, there's an [explanation](http://pandas.pydata.org/pandas-docs/stable/missing_data.html#calculations-with-missing-data) of how `NaN`s are treated in each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# one nan value\n",
    "s1 = pd.Series([0.9,0.6,np.nan,0.12])\n",
    "# zero instead of nan\n",
    "s2 = pd.Series([0.9,0.6, 0,    0.12])\n",
    "# no value\n",
    "s3 = pd.Series([0.9,0.6,       0.12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of each `Series` is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.405"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the sum is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.62"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.62"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.62"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be confusing, taking into account that `np.nan`+`int`=`np.nan` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be **up to the analyst to decide how empty values are handled**, since a **default imputation** may yield unexpected or **erroneous results** for reasons that are hard to trace.\n",
    "\n",
    "Pandas is not consistent with this treatment in contraposition with R, where most of the methods provide to the analysis the option of how to handle NA.\n",
    "\n",
    "We already learnt how to deal with `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
 n rows\n",
    "nulls = df.isnull().sum(axis=1)\n",
    "nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can check, how well informed is a variable. We will have to decide what to do with null values.\n",
    "\n",
    "For example, if they are all grouped within the same column, we would probably drop the whole column as we can consider it **not-well informed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see which columns have nans and its percentage\n",
    "df.isnull().sum() / df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Values\n",
    "\n",
    "Beyond nulls, we may find other types of special values that have to be handled, prior to any further manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `np.inf`\n",
    "Sometimes during the process of our dataset we may introduce errors, like 0 divisons or other arithmetic operations that result in a `np.inf` or `-np.inf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "s = pd.Series(np.arange(-5,5))\n",
    "inv_s = 1/s\n",
    "inv_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value may not be an error at all, even more, we may want to visualize or work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "pd.DataFrame({\"s\": s, \"inv_s\": 1/s}).plot(\"s\",\"inv_s\",\n",
    "                                         kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in general, we won't be able to handle it as `np.inf` in further steps, so we have to, first of all, detect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where 1/s is finite\n",
    "(1/s)[(1/s).map(lambda x: np.isfinite(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many finite values we have\n",
    "np.isfinite(1/s).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where 1/s is finite\n",
    "np.isinf(1/s).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with nans, we can select `np.inf` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1/s)[np.isinf(1/s)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `pd.NaT`\n",
    "\n",
    "\n",
    "We have seen also `pd.NaT` value for `timedeltas`. We can deal with these values in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(pd.NaT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, using `pd.isnull()` we can handle all special, non informed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(pd.Series([pd.NaT,np.nan,None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(from [Pandas API Docs](http://pandas.pydata.org/pandas-docs/stable/missing_data.html#values-considered-missing))*\n",
    "\n",
    "One has to be careful that in `numpy`, `np.nan` values don’t compare using the equal operator, but `None` do. \n",
    "\n",
    "Note that `pandas`/`numpy` uses the fact that `np.nan` != `np.nan`, and treats `None` like `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None == None, np.nan == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we can't use == to compare NaNs\n",
    "s = pd.Series([None,np.nan])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(s == None).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(s == np.nan).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(s.isnull()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Localization\n",
    "\n",
    "In general, errors are detected during Exploratory Data Analysis task, where values are represented using several tools, including plots, tables, etc... \n",
    "\n",
    "We will see how to use `matplotlib` library to explore and visualize data. \n",
    "\n",
    "Now we will focus on learning how to select these erroneous values and eventually modifying it to make our dataset consistent.\n",
    "\n",
    "Imagine we have a variable, let's say a sensor, and after analyzing the data you find that it has negative values\n",
    "\n",
    "It is impossible that real data has negative values, because the data has been recorded in summer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagine we have the following data\n",
    "data = np.random.normal(0,20,1000)\n",
    "pd.DataFrame(data).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a simple arithmetic operation to select all values in negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data<0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we don't know what happened with our sensor, we may assign a `np.nan` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data<0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is a solution that we have to tackle in the future, as if we want to perform further analysis, these values should be deleted or reassigned to other values (imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[pd.Series(data).isnull()]=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Operators: `any` and `all`\n",
    "\n",
    "There are two methods that can be applied over `Boolean` `Series` and `DataFrames` that are interesting for error detection:\n",
    "* `Series.any()`: returns a Boolean value, telling that there exists at least one `True` value in the Series\n",
    "* `Series.all()`: returns a Boolean value, telling that all values are `True`\n",
    "\n",
    "Let's see an example to illustrate.\n",
    "\n",
    "We create two normal random variables. One with mean 0 and std 1, and the other one with mean 6 and std 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"a\" : np.random.normal(0,1, 100),\n",
    "        \"b\" : np.random.normal(6,1, 100)\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is very unlikely that variable `b` contains any negative value, let's check if this holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df<0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the parameter `axis` to make Boolean checking in rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df<0).any(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Series.all()` method works analogously to `Series.any()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df>0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "There is a vast body of literature on outlier detection, and several definitions of\n",
    "outlier exist. \n",
    "\n",
    "A general definition: an outlier in a data set as an observation (or set of observations) which appear to be inconsistent with that set of data. \n",
    "\n",
    "Note that:\n",
    "\n",
    "* Outliers do not equal errors\n",
    "* They should be detected, but not necessarily removed\n",
    "* Their inclusion in the analysis is a statistical decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example (data taken from [here](http://socr.ucla.edu/docs/resources/SOCR_Data/SOCR_Data_Dinov_020108_HeightsWeights.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_df = pd.read_csv(\"../data/heigths_weights_dummy.tsv\",sep=\"\\t\",index_col=0)\n",
    "#chek if there exist nulls\n",
    "num_nulls = hw_df.isnull().any(axis=1).sum()\n",
    "if num_nulls > 0:\n",
    "    print(\"Warning: there are nulls\")\n",
    "else:\n",
    "    display(hw_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can name columns properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_df.rename(columns={\"Height(Inches)\": \"height\",\"Weight(Pounds)\" : \"weight\"}, inplace=True)\n",
    "# equivalent to \n",
    "# hw_df.columns = [\"height\", \"weight\"]\n",
    "\n",
    "hw_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that data units are in Imperial system (inches and pounds), so we probably want to convert it to the metric system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to metric\n",
    "hw_df[\"height\"] = hw_df[\"height\"] * 2.54\n",
    "hw_df[\"weight\"] = hw_df[\"weight\"] * 0.453592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "hw_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset with weights and heights of people. Using describe we can see that:\n",
    "* height average is 175.07 cm\n",
    "* weight average is 63.54 kg\n",
    "    \n",
    "However we have two (max) values (maybe not the same), that clearly overpass the mean: 246.76 and 218.80 respectively.\n",
    "\n",
    "These values are considered outliers, because they are extreme and unusual values.\n",
    "\n",
    "**How can we detect outliers?**\n",
    "\n",
    "For more or less unimodal and symmetrically distributed data, Tukey's box-and-whisker\n",
    "method for outlier detection is often appropriate. \n",
    "\n",
    "By unimodal we mean that the data is centered around the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = np.random.normal(3, 1, 100)\n",
    "n2 = np.random.normal(-3, 1, 100)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "pd.concat([pd.Series(n1), pd.Series(n2)]).plot(kind=\"kde\", ax=axes[0], title=\"Bimodal distribution\")\n",
    "pd.Series(n1).plot(kind=\"kde\", ax=axes[1], title=\"Unimodal distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By symmetric we mean that the data distribution is mirrored from left to right. By definition, a symmetric distribution is never a skewed distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = np.random.normal(3, 1, 100)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "pd.Series(n1).where(n1>3).plot(kind=\"kde\", ax=axes[0], title=\"Skewed distribution\")\n",
    "pd.Series(n1).plot(kind=\"kde\", ax=axes[1], title=\"Symmetric distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tukey's box-and-whisker\n",
    "\n",
    "The Tukey's box-and-whisker  method, an observation is an outlier when it is larger than the so-called **\"whiskers\"** of the set of observations.\n",
    "\n",
    "It is a common plotting tool also named **boxplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_df.boxplot(column=[\"height\"],return_type=\"axes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How whiskers are calculated\n",
    "\n",
    "Whiskers are considered limit values, and these values will define the line between common values and outliers. For any variable we will have two whiskers:\n",
    "\n",
    "* **Upper whisker:** it is computed by adding 1.5 times the interquartile range (IQR) to the third quartile and rounding to the nearest lower observation \n",
    "\n",
    "* **Lower whisker:** it is computed by resting 1.5 times the interquartile range (IQR) to the first quartile and rounding to the nearest higher observation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the IQR?\n",
    "\n",
    "* Quantiles are cutpoints dividing the range of a probability distribution into contiguous intervals with equal probabilities\n",
    "* A percentile (or a centile) is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall\n",
    "* The 25th percentile is also known as the first quartile (Q1), the 50th percentile as the median or second quartile (Q2), and the 75th percentile as the third quartile (Q3)\n",
    "* In general, percentiles and quartiles are specific types of quantiles\n",
    "* The IQR range is basically the rest between 3rd quartile and 1st quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantiles are proportional 4 divisions of the data\n",
    "# We can compute the quantiles easily with pandas.DataFrame.quantile()\n",
    "# note that the result is a DataFrame\n",
    "hw_df.quantile([.25,.5,.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as stated, the IQR is the division between 1st and 3rd quantiles\n",
    "# this function will compute the rest\n",
    "iqr_func = lambda x: x.iloc[1] - x.iloc[0]\n",
    "# note that we just select the 1st and 3rd quantiles\n",
    "iqr = hw_df.quantile([.25,.75]).apply(iqr_func)\n",
    "iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add iqr*1.5  to both quartiles\n",
    "high_out = hw_df.quantile([.75]) + iqr*1.5 \n",
    "low_out = hw_df.quantile([.25]) - iqr*1.5\n",
    "\n",
    "print(\"High whiskers:\")\n",
    "display(high_out)\n",
    "print(\"Lower whiskers\")\n",
    "display(low_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Find upper outliers:\")\n",
    "display(hw_df[hw_df[\"height\"] > high_out.loc[0.75,\"height\"]])\n",
    "\n",
    "print(\"Find lower outliers:\")\n",
    "display(hw_df[hw_df[\"height\"] < low_out.loc[0.25,\"height\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The boxplot show us whiskers and outliers\n",
    "hw_df.boxplot(column=[\"height\"],return_type=\"axes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hw_df[hw_df[\"weight\"] > high_out.loc[0.75,\"weight\"]])\n",
    "print(hw_df[hw_df[\"weight\"] < low_out.loc[0.25,\"weight\"]] )\n",
    "\n",
    "hw_df.boxplot(column=[\"weight\"],return_type=\"axes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Outliers\n",
    "\n",
    "(*from https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/#three*)\n",
    "\n",
    "Whenever we come across outliers, the ideal way to tackle them is to find out the reason of having these outliers. \n",
    "\n",
    "The method to deal with them would then depend on the reason of their occurrence. Causes of outliers can be classified in two broad categories:\n",
    "\n",
    "* **Artificial (Error) / Non-natural**\n",
    "* **Natural.**\n",
    "\n",
    "Let’s understand various types of outliers in more detail:\n",
    "\n",
    "* **Data Entry Errors:**- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data. For example: Annual income of a customer is <span>\\$</span>100,000. Accidentally, the data entry operator puts an additional zero in the figure. Now the income becomes \\$1,000,000 which is 10 times higher. Evidently, this will be the outlier value when compared with rest of the population.\n",
    "\n",
    "* **Measurement Error:** It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty. For example: There are 10 weighing machines. 9 of them are correct, 1 is faulty. Weight measured by people on the faulty machine will be higher / lower than the rest of people in the group. The weights measured on faulty machine can lead to outliers.\n",
    "\n",
    "* **Experimental Error:** Another cause of outliers is experimental error. For example: In a 100m sprint of 7 runners, one runner missed out on concentrating on the ‘Go’ call which caused him to start late. Hence, this caused the runner’s run time to be more than other runners. His total run time can be an outlier.\n",
    "\n",
    "* **Intentional Outlier:** This is commonly found in self-reported measures that involves sensitive data. For example: Teens would typically under report the amount of alcohol that they consume. Only a fraction of them would report actual value. Here actual values might look like outliers because rest of the teens are under reporting the consumption.\n",
    "\n",
    "* **Data Processing Error:** Whenever we perform data mining, we extract data from multiple sources. It is possible that some manipulation or extraction errors may lead to outliers in the dataset.\n",
    "\n",
    "* **Sampling error:** For instance, we have to measure the height of athletes. By mistake, we include a few basketball players in the sample. This inclusion is likely to cause outliers in the dataset.\n",
    "\n",
    "* **Natural Outlier:** When an outlier is not artificial (due to error), it is a natural outlier. For instance: In my last assignment with one of the renowned insurance company, I noticed that the performance of top 50 financial advisors was far higher than rest of the population. Surprisingly, it was not due to any error. Hence, whenever we perform any data mining activity with advisors, we used to treat this segment separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 95 percentile rule\n",
    "It is also common to get rid of everything higher than 95 percentile and lower than 5 percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data with histograms\n",
    "hw_df[\"height\"].hist(bins=hw_df.shape[0])\n",
    "print(\".95 and .5 percentiles\")\n",
    "display(hw_df[\"height\"].quantile([.05,.95]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save percentiles in vars\n",
    "hl = hw_df[\"height\"].quantile([.95])\n",
    "ll = hw_df[\"height\"].quantile([.05])\n",
    "#filter and plot\n",
    "hw_df[\"height\"][(hw_df[\"height\"] < hl.iloc[0]) & (hw_df[\"height\"] > ll.iloc[0])].hist(bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_df[\"weight\"].hist(bins=hw_df.shape[0])\n",
    "hw_df[\"weight\"].quantile([.05,.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Correction\n",
    "\n",
    "### Simple transformation\n",
    "\n",
    "We can transform data using data from two different dataset with same labels.\n",
    "\n",
    "In the following dataset there's an error, and variable a has to be the double for observations labelled as b in variable b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"a\":[1,2,3,4,5],\"b\":[\"a\",\"a\",\"a\",\"b\",\"b\"]})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([1,2],index=[\"a\",\"b\"])\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(pd.DataFrame(s1), on=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique can be useful for example when there's an error in a variable, but only for certain values.\n",
    "\n",
    "Let's see an example. The following dataset has information about Real Estate Transactions in Sacramento City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Sacramentorealestatetransactions.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.dtypes).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**  somebody tells you that `Residential` houses the unit is in square meters, instead of square feet.\n",
    "**Approach:**\n",
    "1. Generate a `Series` object with a multiplier factors\n",
    "2. Propagate `multiplier_factor` along `df` using `DataFrame.join`\n",
    "3. Multiply `sq__ft` variable by `multiplier_factor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate a new column with a multiplier factor\n",
    "multiplier_factor = pd.Series(1, \n",
    "                              index=df.type.unique(), \n",
    "                              dtype=float, \n",
    "                              name=\"multiplier_factor\"\n",
    "                             )\n",
    "multiplier_factor[\"Residential\"] = 0.092903\n",
    "multiplier_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate multiplier_factor along df using DataFrame.join\n",
    "conversion_df = df.join(pd.DataFrame(multiplier_factor), on=\"type\").loc[:,[\"type\", \"sq__ft\", \"multiplier_factor\"]]\n",
    "conversion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sq__ft\"] = conversion_df.sq__ft*conversion_df.multiplier_factor\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "We can define imputation as the process of estimating or deriving values for fields where **data is missing**. \n",
    "\n",
    "There is a vast body of literature on imputation methods and it goes beyond the scope of this course to discuss all of them.\n",
    "\n",
    "Here, we will use pandas to calculate values to impute and assign them to empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\"a\":[0.8,0.4,1.8,np.nan],\n",
    "                   \"b\":[2.3,np.nan,np.nan,5.6],\n",
    "                   \"c\":[2.4,3.2,1.1,4.5],\n",
    "                   \"d\":[np.nan,4.4,1.8,np.nan],\n",
    "                   \"tag\":[\"a\",\"a\",\"b\",\"b\"]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily replace **missing** values using the `DataFrame.fillna()` method. This method takes a default value as parameter and replaces `np.nan` values with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has the same result as `df.replace(np.nan, 0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `DataFrame.fillna`\n",
    "However `DataFrame.fillna` has some interesting parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* propagate non-null values forward or backward. This means that when a `NaN` is found, the previous (`ffill`) or next (`bfill`) values are assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Replace `NaN` elements with default values per column using a `dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({\"a\":0, \"b\": 1, \"d\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Only replace the first NaN element using `limit` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Series.interpolate`\n",
    "\n",
    "Sometimes we want to do something smarter, than just replace for a fixed value.\n",
    "\n",
    "Imagine that you have a dataset, where you know that a variable has a relationship with respect to previous and next observation.\n",
    "\n",
    "It would be better, if instead of just assigning a default value, or deleting the missing value, assign a value **in between** the two values.\n",
    "\n",
    "For example, let's imagine we have a temperature sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = pd.Series([10.2, 10.3, 10.5, np.nan, 10.9, np.nan, 11.1, 10.8, 10.4, 10.2, np.nan, 10])\n",
    "temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=temp_data.index, y=temp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply, delete these values, however if our dataset has a specific sampling frequency it would somehow break the structure.\n",
    "\n",
    "Another approach would be to **interpolate**, guessing a relation between values.\n",
    "\n",
    "If no physicist in the room we will use the default value for this relation which is `method='linear'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.interpolate(inplace=True, )\n",
    "temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(temp_data, 'bo-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `DataFrame.dropna`\n",
    "A more drastic approach would be to get rid of these observations or variables that contain an nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\"a\":[0.8,0.4,1.8,np.nan],\n",
    "                   \"b\":[2.3,np.nan,np.nan,5.6],\n",
    "                   \"c\":[2.4,3.2,1.1,4.5],\n",
    "                   \"d\":[np.nan,4.+4,1.8,np.nan],\n",
    "                   \"tag\":[\"a\",\"a\",\"b\",\"b\"]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with `NaN`\n",
    "df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imputation Methods\n",
    "\n",
    "#### Mean Imputation\n",
    "\n",
    "If we know nothing about a variable, we can use the mean of a (numeric) variable\n",
    "$$ \\hat{x}_i = \\bar{x}$$\n",
    "where the $\\bar{x}$ is the imputation value and the mean is taken over the observed values. \n",
    "\n",
    "The usability of this model is limited since it obviously causes a bias in spread measures, estimated from the sample after imputation. \n",
    "\n",
    "In principle one can use other measures of centrality. For example, imputation of the mean or median can be done as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.DataFrame({\"a\":[0.8,0.4,1.8,np.nan],\n",
    "                   \"b\":[2.3,np.nan,np.nan,5.6],\n",
    "                   \"c\":[2.4,3.2,1.1,4.5],\n",
    "                   \"d\":[np.nan,4.4,1.8,np.nan],\n",
    "                   \"tag\":[\"a\",\"a\",\"b\",\"b\"]})\n",
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check where is a null\n",
    "nulls = original_df.isnull().sum(axis=1)\n",
    "original_df[nulls == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which columns have nans\n",
    "original_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which columns have nans\n",
    "display(original_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see means\n",
    "original_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute means\n",
    "original_df.fillna(original_df.mean())[nulls == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computed Values Imputation\n",
    "\n",
    "But, we can do it better.\n",
    "\n",
    "We have a `tag` variable, and maybe we can use this information to:\n",
    "1. Compute the mean of items belonging to each `tag` class\n",
    "2. Assign the computed values to missing values in each variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `DataFrame.pivot_table`\n",
    "\n",
    "Our mission is to convert `tag` variable into the `index` of the dataset,\n",
    "\n",
    "However, we have multiple rows with `a` and `b` values, so we have to aggregate this values somehow.\n",
    "\n",
    "`DataFrame.pivot_table` is the appropriate method for this task, it takes 3 parameters:\n",
    "* `index`: which variable will be the new index\n",
    "* `values`: which values will be the columns\n",
    "* `aggfunc`: how multiple values will be aggregated\n",
    "\n",
    "In our case we will aggregate using `np.mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df.pivot_table(index=\"tag\",values=[\"a\",\"b\",\"c\",\"d\"],aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to fill it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_means = original_df.pivot_table(index=\"tag\",values=[\"a\",\"b\",\"c\",\"d\"],aggfunc=np.mean)\n",
    "tag_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first imputation\n",
    "original_df.loc[(original_df.tag==\"a\") & (original_df.b.isnull()),\"b\"] = tag_means.loc[\"a\",\"b\"]\n",
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second imputation\n",
    "original_df.loc[(original_df.tag==\"a\") & (original_df.d.isnull()),\"d\"] = tag_means.loc[\"a\",\"d\"]\n",
    "display(original_df[nulls == 1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
