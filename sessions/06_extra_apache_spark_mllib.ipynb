{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark MLlib\n",
    "\n",
    "MLlib is Spark’s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:\n",
    "\n",
    "* **ML Algorithms:** common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "* **Featurization:** feature extraction, transformation, dimensionality reduction, and selection\n",
    "* **Pipelines:** tools for constructing, evaluating, and tuning ML Pipelines\n",
    "* **Persistence:** saving and load algorithms, models, and Pipelines\n",
    "* **Utilities:** linear algebra, statistics, data handling, etc.\n",
    "\n",
    "You can find 2 Machine Libraries APIs in Spark:\n",
    "    * spark.mllib: it is the RDD-based API\n",
    "    * spark.ml: it is the DataFrame-based API\n",
    "    \n",
    "**spark.ml** is the primary ML library in Spark. **spark.mllib** is in maintenance mode. This means that it can be used and it will have bug fixes but will not have any  new features.\n",
    "\n",
    "As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.\n",
    "\n",
    "DataFrames provide a more user-friendly API than RDDs. The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Used\n",
    "\n",
    "We will use different data sets:\n",
    "\n",
    "* **Papers Dataset:**\n",
    "\n",
    "This is a dataset containing conferences, authors and paper titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(path = '../data/papers.csv', header = False,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+--------+--------------------+\n",
      "|_c0| _c1|_c2|     _c3|                 _c4|\n",
      "+---+----+---+--------+--------------------+\n",
      "|  0|1996|  3|3CONLINE|Shene, A Comparat...|\n",
      "|  1|1994|  1|3CONLINE|Janik, Textbooks ...|\n",
      "|  2|1999|  4|THESTATE|Fink & Mao, The 8...|\n",
      "|  3|2000| 17|    AAAI|Achlioptas & Gome...|\n",
      "|  4|2000| 17|    AAAI|Bejar & Manya, So...|\n",
      "+---+----+---+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Black Friday Dataset**\n",
    "\n",
    "source: https://datahack.analyticsvidhya.com/contest/black-friday\n",
    "\n",
    "The data set also contains customer demographics (age, gender, marital status, city_type, stay_in_current_city), product details (product_id and product category) and Total purchase_amount from last month.\n",
    "\n",
    "\n",
    "| Variable | Definition |\n",
    "|----------|------------|\n",
    "| User_ID | User ID|\n",
    "| Product_ID|\tProduct ID|\n",
    "| Gender|\tSex of User|\n",
    "| Age|\tAge in bins|\n",
    "| Occupation|\tOccupation (Masked)|\n",
    "| City_Category|\tCategory of the City (A,B,C)|\n",
    "| Stay_In_Current_City_Years|\tNumber of years stay in current city|\n",
    "| Marital_Status|\tMarital Status|\n",
    "| Product_Category_1|\tProduct Category (Masked)|\n",
    "| Product_Category_2|\tProduct may belongs to other category also (Masked)|\n",
    "| Product_Category_3|\tProduct may belongs to other category also (Masked)|\n",
    "| Purchase|\tPurchase Amount (Target Variable)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_train = spark.read.csv(path = '../data/blackfriday_train.csv', header = True,inferSchema = True)\n",
    "bf_test = spark.read.csv(path = '../data/blackfriday_test.csv', header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Product_ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Occupation: integer (nullable = true)\n",
      " |-- City_Category: string (nullable = true)\n",
      " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
      " |-- Marital_Status: integer (nullable = true)\n",
      " |-- Product_Category_1: integer (nullable = true)\n",
      " |-- Product_Category_2: integer (nullable = true)\n",
      " |-- Product_Category_3: integer (nullable = true)\n",
      " |-- Purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bf_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames Manipulations\n",
    "source: https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "\n",
    "### How to see datatype of columns?\n",
    "\n",
    "To see the types of columns in DataFrame, we can use the printSchema. printSchema() on a DataFrame will show the schema in a tree format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Show first n observation?\n",
    "\n",
    "We can use head operation to see first n observation (say, 5 observation). Head operation in PySpark is returns a list of Rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0, _c1=1996, _c2='3', _c3='3CONLINE', _c4='Shene, A Comparative Study of Linked List Sorting Algorithms'),\n",
       " Row(_c0=1, _c1=1994, _c2='1', _c3='3CONLINE', _c4='Janik, Textbooks for Teaching C++'),\n",
       " Row(_c0=2, _c1=1999, _c2='4', _c3='THESTATE', _c4='Fink & Mao, The 85 Ways to Tie a Tie:  The Science and Aesthetics of Tie Knots'),\n",
       " Row(_c0=3, _c1=2000, _c2='17', _c3='AAAI', _c4='Achlioptas & Gomes & Kautz & Selman, Generating Satisfiable Problem Instances'),\n",
       " Row(_c0=4, _c1=2000, _c2='17', _c3='AAAI', _c4='Bejar & Manya, Solving the Round Robin Problem Using Propositional Logic')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the result formatted as DataFrames output, we can use the show operation. \n",
    "\n",
    "We can pass the argument truncate = True to truncate the result (the row won't be shown completely).\n",
    "\n",
    "Note that show does not return any data, just shows the DataFrame contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+--------+--------------------+\n",
      "|_c0| _c1|_c2|     _c3|                 _c4|\n",
      "+---+----+---+--------+--------------------+\n",
      "|  0|1996|  3|3CONLINE|Shene, A Comparat...|\n",
      "|  1|1994|  1|3CONLINE|Janik, Textbooks ...|\n",
      "+---+----+---+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df.show(2,truncate= True)))\n",
    "\n",
    "#df.show(2,truncate= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Count the number of rows in DataFrame?\n",
    "\n",
    "We can use count operation to count the number of rows in DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123315"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many columns do we have in train and test files along with their names?\n",
    "\n",
    "For getting the columns name we can use columns on DataFrame, similar to what we do for getting the columns in pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, ['_c0', '_c1', '_c2', '_c3', '_c4'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns), df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get the summary statistics (mean, standard deviance, min ,max , count) of numerical columns in a DataFrame?\n",
    "\n",
    "describe operation is use to calculate the summary statistics of numerical column(s) in DataFrame. If we don’t specify the name of columns it will calculate summary statistics for all numerical columns present in DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+--------+--------------------+\n",
      "|summary|              _c0|               _c1|               _c2|     _c3|                 _c4|\n",
      "+-------+-----------------+------------------+------------------+--------+--------------------+\n",
      "|  count|           123315|            123315|            123315|  123315|              123314|\n",
      "|   mean|          61657.0|1995.1891578477882| 90.64012293936854|    null|                null|\n",
      "| stddev|35598.11855983403| 25.75542351380241|344.61551232986926|    null|                null|\n",
      "|    min|                0|               196|                 1|3CONLINE|\"Baker, \"\"Natural...|\n",
      "|    max|           123314|              2008|                 Z|       d|withheld, FREE Ha...|\n",
      "+-------+-----------------+------------------+------------------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `describe()` is a transformation, so the result is a DataFrame that we can `collect` or transform again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _c0: string, _c1: string, _c2: string, _c3: string, _c4: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that, describe operation is working for String type column but the output for mean, stddev are null and min & max values are calculated based on ASCII value of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(\"_c0\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find the number of distinct product in train and test files?\n",
    "\n",
    "The distinct operation can be used here, to calculate the number of distinct rows in a DataFrame. Let’s apply distinct operation to calculate the number of distinct product in df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123315"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_elements = df.select('_c0').distinct().collect()\n",
    "len(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('_c0').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|      _c3|\n",
      "+---------+\n",
      "|AJPHYSICS|\n",
      "|   ICVLSI|\n",
      "|  NTMATHU|\n",
      "|  ATHCOMP|\n",
      "|  NOTICES|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if we want to get the different number of conferences\n",
    "df.select(\"_c3\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if I want to calculate pair wise frequency of categorical columns?\n",
    "\n",
    "We can use crosstab operation on DataFrame to calculate the pair wise frequency of columns. Let’s apply crosstab operation on `Age[years]` and `Sex` columns of df DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(path = '../data/people.csv', header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-----+\n",
      "|Age[years]_Sex| female| male|\n",
      "+--------------+-------+-----+\n",
      "|          42.0|      1|    0|\n",
      "|          37.0|      0|    1|\n",
      "|          29.0|      0|    1|\n",
      "|          61.0|      1|    0|\n",
      "|          33.0|      0|    1|\n",
      "|          77.0|      1|    0|\n",
      "|          32.0|      1|    0|\n",
      "|          45.0|      0|    1|\n",
      "|          18.0|      0|    1|\n",
      "|          19.0|      0|    1|\n",
      "+--------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.crosstab('Age[years]', 'Sex').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, the first column of each row will be the distinct values of `Age` and the column names will be the distinct values of `Sex`. The name of the first column will be `Age[years]_Sex`. Pair with no occurrences will have zero count in contingency table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What If I want to get the DataFrame which won’t have duplicate rows of given DataFrame?\n",
    "\n",
    "We can use dropDuplicates operation to drop the duplicate rows of a DataFrame and get the DataFrame which won’t have duplicate rows. \n",
    "\n",
    "If we apply this on two columns `Sex` and `Eye Color` of df2 and get the all unique rows for these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|    Sex|Eye Color|\n",
      "+-------+---------+\n",
      "| female|    brown|\n",
      "|   male|    green|\n",
      "|   male|     blue|\n",
      "| female|     blue|\n",
      "| female|     gray|\n",
      "|   male|    green|\n",
      "| female|    brown|\n",
      "|   male|    brown|\n",
      "|   male|    green|\n",
      "|   male|     gray|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('Sex', 'Eye Color').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|    Sex|Eye Color|\n",
      "+-------+---------+\n",
      "|   male|    brown|\n",
      "| female|     gray|\n",
      "|   male|     blue|\n",
      "| female|     blue|\n",
      "|   male|    green|\n",
      "| female|    brown|\n",
      "|   male|     gray|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('Sex', 'Eye Color').dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select('Sex', 'Eye Color').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select('Sex', 'Eye Color').dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if I want to drop the all rows with null value?\n",
    "\n",
    "The `dropna` operation can be use here. To drop row from the DataFrame it consider three options.\n",
    "\n",
    "* `how`– `any` or `all`. If `any`, drop a row if it contains any nulls. If `all`, drop a row only if all its values are null.\n",
    "* `thresh` –  `int`, default `None` If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "* `subset` – optional list of column names to consider.\n",
    "\n",
    "Let’t drop null rows in df2 with default parameters and count the rows in output DataFrame. Default options are `any`, `None`, `None` for how, `thresh`, `subset` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bf_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166821"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bf_train.dropna().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if I want to fill the null values in DataFrame with constant number?\n",
    "\n",
    "Use fillna operation here. The fillna will take two parameters to fill the null values.\n",
    "\n",
    "* `value`:\n",
    "    * It will take a dictionary to specify which column will replace with which value.\n",
    "    * A value (int , float, string) for all columns.\n",
    "* `subset`: Specify some selected columns.\n",
    "\n",
    "Let’s fill `-1` inplace of null values in train DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_train.fillna(-1).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If I want to filter the rows in train which has Purchase more than 15000?\n",
    "\n",
    "We can apply the filter operation on Purchase column in `bf_train` DataFrame to filter out the rows with values more than 15000. \n",
    "\n",
    "We need to pass a condition. \n",
    "\n",
    "Let’s apply filter on `Purchase` column in train DataFrame and print the number of rows which has more purchase than 15000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_train.filter(bf_train.Purchase > 15000).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find the mean of each age group in train?\n",
    "\n",
    "The `groupby` operation can be used here to find the mean of `Purchase` for each age group in `bf_train`. Let’s see how can we get the mean purchase for the `Age` column train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_train.groupby('Age').agg({'Purchase': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply `sum`, `min`, `max`, `count` with `groupby` when we want to get different summary insight each group. \n",
    "\n",
    "Let’s take one more example of `groupby` to count the number of rows in each `Age` group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_train.groupby('Age').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a sample DataFrame from the base DataFrame?\n",
    "\n",
    "We can use sample operation to take sample of a DataFrame. \n",
    "\n",
    "The `sample` method on DataFrame will return a DataFrame containing the sample of base DataFrame. The sample method will take 3 parameters.\n",
    "\n",
    "* `withReplacement = True` or `False` to select a observation with or without replacement.\n",
    "* `fraction = x`, where `x = .5` shows that we want to have 50% data in sample DataFrame.\n",
    "* `seed` to reproduce the result\n",
    "\n",
    "Let’s create the two DataFrame `t1` and `t2` from `bf_train`, both will have 20% sample of train and count the number of rows in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = bf_train.sample(False, 0.2, 42)\n",
    "t2 = bf_train.sample(False, 0.2, 43)\n",
    "t1.count(),t2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to apply map operation on DataFrame columns?\n",
    "\n",
    "We can apply a function on each row of DataFrame using `map` operation. After applying this function, we get the result in the form of `RDD`. Let’s apply a `map` operation on `User_ID` column of `bf_train` and print the first 5 elements of mapped `RDD(x,1)` after applying the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_train.select('User_ID').rdd.map(lambda x:(x,1)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to sort the DataFrame based on column(s)?\n",
    "\n",
    "We can use `orderBy` operation on DataFrame to get sorted output based on some column. The `orderBy` operation take two arguments.\n",
    "\n",
    "* List of columns.\n",
    "* ascending = True or False for getting the results in ascending or descending order(list in case of more than two columns )\n",
    "\n",
    "Let’s sort the train DataFrame based on `Purchase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_train.orderBy(bf_train.Purchase.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to add the new column in DataFrame?\n",
    "\n",
    "We can use `withColumn` operation to add new column (we can also replace) in base DataFrame and return a new DataFrame. The `withColumn` operation will take 2 parameters.\n",
    "\n",
    "* Column name which we want add /replace.\n",
    "* Expression on column.\n",
    "\n",
    "Let’s see how `withColumn` works. To calculate new column name `Purchase_new` in `bf_train` which is calculated by dviding `Purchase` column by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_train.withColumn('Purchase_new', bf_train.Purchase /2.0).select('Purchase','Purchase_new').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use functions with withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Purchase_cat|\n",
      "+------------+\n",
      "|   expensive|\n",
      "|       cheap|\n",
      "|   expensive|\n",
      "|   expensive|\n",
      "|   expensive|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "to_cat = udf(lambda x: \"cheap\" if x > 15000 else \"expensive\", StringType())\n",
    "bf_train.withColumn('Purchase_cat', to_cat(bf_train[\"Purchase\"])).select('Purchase_cat').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to drop a column in DataFrame?\n",
    "\n",
    "To drop a column from the DataFrame we can use `drop` operation. Let’s `drop` the column called `Comb` from the `bf_test` and get the remaining columns in `bf_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Product_ID',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'Occupation',\n",
       " 'City_Category',\n",
       " 'Stay_In_Current_City_Years',\n",
       " 'Marital_Status',\n",
       " 'Product_Category_1',\n",
       " 'Product_Category_2',\n",
       " 'Product_Category_3']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bf_test.drop('Comb').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to split a DataFrame into two new DataFrames\n",
    "\n",
    "Sometime we will want to have a DataFrame divided into separate parts separated randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110873"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12442"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebraic Structures\n",
    "### Dense and Sparse Vectors\n",
    "\n",
    "* A vector is a one-dimensional array of elements. \n",
    "\n",
    "    The natural Python implementation of a vector is as a one-dimensional list. However, in many applications, the elements of a vector have mostly zero values. Such a vector is said to be **sparse**. \n",
    "    \n",
    "    It is inefficient to use a one-dimensional list to store a sparse vector. It is also inefficient to add elements whose values are zero in forming sums of sparse vectors. Consequently, we should choose a different representation.\n",
    "\n",
    "\n",
    "* A dense vector is the most natural implementation, using one-dimensional list\n",
    "\n",
    "A sparse vector is represented by two parallel arrays: indices and values. Zero entries are not stored. A dense vector is backed by a double array representing its entries. For example, a vector [1., 0., 0., 0., 0., 0., 0., 3.] can be represented in the sparse format as (7, [0, 6], [1., 3.]), where 7 is the size of the vector, as illustrated below:\n",
    "\n",
    "![alt text](https://databricks.com/wp-content/uploads/2014/07/xAJI2gUHspdUO4rO3JxsGPlHCsUvIZc9xI08QJA_14sebOFMDFMTSvbYi1c4AaPS-rh7Ly-FBJukdGxOo7mjKj9q4wb1ehZXfFFZHOLdn2MQGhLjtooF7Cm053PbwvXHUg.png)\n",
    "(source: https://databricks.com/blog/2014/07/16/new-features-in-mllib-in-spark-1-0.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector, DenseVector, Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 1.0, 2: 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv1 = SparseVector(3, [0, 2], [1.0, 3.0])\n",
    "sv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 3.0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv1 = DenseVector([1.0, 3.0])\n",
    "dv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have Sparse and Dense Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrices.dense(2,2,[1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_mat = Matrices.sparse(2,2,[1,2,3],[0,1],[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_mat = Matrices.dense(2,2,[1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to, instead of represent features as variables, to represent all instance variables as a vector, which indeed can be sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_df = sc.parallelize([\n",
    "  (1, SparseVector(10, {1: 1.0, 2: 1.0, 3: 2.0, 4: 1.0, 5: 3.0})),\n",
    "  (2, SparseVector(10, {9: 100.0})),\n",
    "  (3, SparseVector(10, {1: 1.0})),\n",
    "]).toDF([\"row_num\", \"features\"])\n",
    "sparse_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_df = sc.parallelize([\n",
    "  (1, DenseVector([1,2,3,4])),\n",
    "  (2, DenseVector([1,2,3,4])),\n",
    "  (3, DenseVector([1,3,4,5])),\n",
    "]).toDF([\"row_num\", \"features\"])\n",
    "dense_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to Exploit Sparsity\n",
    "\n",
    "from: https://databricks.com/blog/2014/07/16/new-features-in-mllib-in-spark-1-0.html\n",
    "\n",
    "For many large-scale datasets, it is not feasible to store the data in a dense format. Nevertheless, for medium-sized data, it is natural to ask when we should switch from a dense format to sparse. In MLlib, a sparse vector requires 12nnz+4 bytes of storage, where nnz is the number of nonzeros, while a dense vector needs 8n bytes, where n is the vector size. So storage-wise, the sparse format is better than the dense format when more than 1/3 of the elements are zero. However, assuming that the data can be fit into memory in both formats, we usually need sparser data to observe a speedup, because the sparse format is not as efficient as the dense format in computation. Our experience suggests a sparsity of around 10%, while the exact switching point for the running time is indeed problem-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "from: https://spark.apache.org/docs/2.3.2/ml-pipeline.html\n",
    "\n",
    "In this section, we introduce and practice with the concept of ML Pipelines.\n",
    "\n",
    "ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines.\n",
    "\n",
    "MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow. This section covers the key concepts introduced by the Pipelines API, where the pipeline concept is mostly inspired by the scikit-learn project.\n",
    "\n",
    "* **DataFrame:** This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.\n",
    "\n",
    "* **Transformer:** A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n",
    "\n",
    "* **Estimator:** An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
    "\n",
    "* **Pipeline:** A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "A Transformer is an abstraction that includes feature transformers and learned models. \n",
    "\n",
    "Technically, a Transformer implements a method `transform()`, which converts one DataFrame into another, generally by appending one or more columns. \n",
    "\n",
    "For example:\n",
    "\n",
    "A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.\n",
    "\n",
    "A learning model might take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators\n",
    "\n",
    "An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. Technically, an Estimator implements a method `fit()`, which accepts a DataFrame and produces a Model, which is a Transformer. \n",
    "\n",
    "For example, a learning algorithm such as LogisticRegression is an Estimator, and calling `fit()` trains a LogisticRegressionModel, which is a Model and hence a Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code examples\n",
    "\n",
    "We will see two examples of how to train Supervised Machine Learning models. Supervised Models try learn from labeled datasets. This means that we have a dataset with some variables for each ocurrence and a label of that occurrence.\n",
    "\n",
    "The main objective is to predict a label for a new occurrence for which we don't have the label.\n",
    "\n",
    "### Regression: Predicting a Continuous Variable\n",
    "\n",
    "In the following example we will load the `auto-mpg.csv` dataset. The description says (http://archive.ics.uci.edu/ml/datasets/Auto+MPG)\n",
    "> \"The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes.\" (Quinlan, 1993)\n",
    "\n",
    "So, first of all, let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df = spark.read.csv(path = '../data/auto-mpg.csv', \n",
    "                         header = True, \n",
    "                         inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- mpg: double (nullable = true)\n",
      " |-- cylinders: integer (nullable = true)\n",
      " |-- displacement: double (nullable = true)\n",
      " |-- horsepower: string (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- acceleration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- origin: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "auto_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main goal is to build a predictive model that takes some input variables representing the vehicle features, and outputs the consumption of the vehicle in miles per gallon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vars = ['cylinders', 'displacement', 'weight', 'acceleration', 'year', 'origin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the consumption we will use a Linear Regression model. We won't go into details about the model itself, but we have to take into account the following considerations:\n",
    "\n",
    "* The model will take as input a `Vector` representing the vehicle characteristics\n",
    "* All input variables must be numeric variables\n",
    "* The Linear Regressor model is an Estimator\n",
    "* A Linear Regressor is a Supervised Machine Learning algorithm\n",
    "* The model obtained is a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the training dataset we will use a VectorAssembler, which is a transformer.\n",
    "\n",
    "VectorAssembler, takes a DataFrame as input, and outputs the same DataFrame with the specified columns in a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[8.0,307.0,3504.0...| 18.0|\n",
      "|[8.0,350.0,3693.0...| 15.0|\n",
      "|[8.0,318.0,3436.0...| 18.0|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols = pred_vars, \n",
    "    outputCol = 'features')\n",
    "train_df = vectorAssembler.transform(auto_df)\n",
    "\n",
    "train_df = train_df.withColumn(\"label\", auto_df[\"mpg\"])\n",
    "\n",
    "train_df = train_df.select(['features', 'label'])\n",
    "train_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a LinearRegression Estimator (skip parameters details). Remember that `lrModel` is a Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.LinearRegressionModel"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# LinearRegression is an Estimator\n",
    "lr = LinearRegression(maxIter=10, \n",
    "                      regParam=0.3, \n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_df)\n",
    "# lrModel will contain a Transformer\n",
    "type(lrModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the predictions over the dataset, we just have to apply the transformer over the features of a certain dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[8.0,307.0,3504.0...|15.516094979306784|\n",
      "|[8.0,350.0,3693.0...| 14.40174603470855|\n",
      "|[8.0,318.0,3436.0...|15.891039618272371|\n",
      "|[8.0,304.0,3433.0...|15.930576188946931|\n",
      "|[8.0,302.0,3449.0...|15.804137052500417|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(train_df.select(['features']))\n",
    "\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: Learning to predict text classes\n",
    "\n",
    "In this second example we will see how to classify text using a Logistic Regression model. The aim of this example is to learn how to concatenate some Transformers in a Pipeline.\n",
    "\n",
    "We will use two transformers and an estimator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizer: it is a Transformer that takes a textual column as input and generates a vector of tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0,\"the cat in the mat is flat\"),\n",
    "    (1,\"the mouse with the hat is nice\")\n",
    "], [\"id\",\"text\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "tok_df = tokenizer.transform(df)\n",
    "\n",
    "tok_df.select(\"words\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CountVectorizer: Convert a list of words into a vector of variables. It does so by converting each word into an index and then at each position (which represents the word in the vector) counts the occurrences of the word in the original list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "\n",
    "counter = count_vec.fit(tok_df)\n",
    "\n",
    "count_df = counter.transform(tok_df)\n",
    "count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LogisticRegression: takes some input variables with a label and constructs a classification model\n",
    "* Pipeline: puts together transformers and estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### Train/Test separation\n",
    "\n",
    "The most straightforward approach to evaluate a supervised model is to split the original dataset into two subsets. \n",
    "\n",
    "* Training subset: this set is used to train a model. Normally, supervised Machine Learning algorithms try to minimize an error value, so algorithms use features and labels from the training dataset to learn a model that minimizes this error.\n",
    "\n",
    "    However, if we expose the algorithm to too much learning we may see that the algorithm begins to memorize occurrences in the training dataset. This is the so called overfitting problem, this problem derives to poor generalization. At the end of the day we have a model that performs very well over the training data, but does a bad prediction into new occurrences.\n",
    "\n",
    "\n",
    "* Test subset: if we separate a group of occurrences that are hidden to the algorithm during training, we can use these occurrences to check how is our model behaving to new occurrences. We can do so because the test subset is a proper labelled set of data, so we can check the difference between the actual label and the predicted label. With this difference we can generate an analysis to see the goodness of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of a Regression Model\n",
    "\n",
    "To evaluate a Regression model we can use the following metrics:\n",
    "\n",
    "* Mean Absolute Error (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^{n} |y_{i}-\\hat{y}_{i}| $$\n",
    " \n",
    "* Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^{n} \\sqrt{(y_{i}-\\hat{y}_{i})^2} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df = spark.read.csv(path = '../data/auto-mpg.csv', \n",
    "                         header = True, \n",
    "                         inferSchema = True)\n",
    "\n",
    "pred_vars = ['cylinders', 'displacement', 'weight', 'acceleration', 'year', 'origin']\n",
    "\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols = pred_vars, \n",
    "    outputCol = 'features')\n",
    "\n",
    "vec_auto_df = vectorAssembler.transform(auto_df)\n",
    "\n",
    "vec_auto_df = vec_auto_df.withColumn(\"label\", auto_df[\"mpg\"])\n",
    "\n",
    "vec_auto_df = vec_auto_df.select(['features', 'label'])\n",
    "\n",
    "train_auto_df, test_auto_df = vec_auto_df.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10, \n",
    "                      regParam=0.3, \n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_auto_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_auto_df = lrModel.transform(test_auto_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"label\", metricName=\"mae\")\n",
    "\n",
    "results = lr_evaluator.evaluate(predicted_auto_df)\n",
    "\n",
    "print(\"R Squared (R2) on test data = %g\" % results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of a Classification Model\n",
    "\n",
    "We will focus on the classification of a binary model. This means that the classification model classifies between two exclusive variables: \"a\" and \"b\" for example.\n",
    "\n",
    "A binary classifier can be seen as a classifier telling if the occurrence belongs to one of the classes, say \"a\". So, when the classifier outputs a true value, it means that the occurrence belongs to class a. If the classifier outputs a false value, it means that it does not belong to class \"a\", hence it belong to class \"b\". So, false outputs mean that the occurrence belongs to class \"b\".\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png)\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "When we classify a new occurrence, we can have the following 4 cases:\n",
    "\n",
    "![alt text](img/wikipedia_tptnfpfn.png)\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "* **True Positive:** The prediction and the actual value are the same, a positive value\n",
    "* **True Negative:** The prediction and the actual value are the same, a negative value\n",
    "* **False Positive:** The prediction and the actual value differ, the actual value is negative but the predicted value is positive\n",
    "* **False Negative:** The prediction and the actual value differ, the actual value is positive but the predicted value is negative\n",
    "\n",
    "Taking these definitions into account we can define the following metrics:\n",
    "\n",
    "* **Accuracy:** among all the sample, how many are correct\n",
    "$$ acc = \\frac{TP+TN}{TP+TN+FP+FN}$$\n",
    "* **Precision:** for those for which the model said as positive, how many of them are correct\n",
    "$$ prec = \\frac{TP}{TP+FP} $$\n",
    "* **Recall:** for those which are actually real, how many of them my model can label correctly\n",
    "$$ rec = \\frac{TP}{TP+FN} $$\n",
    "* **F1 measure:**\n",
    "$$ F = 2 \\cdot \\frac{prec \\cdot acc}{prec + acc} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "iris_df = spark.read.csv(path = '../data/iris.data', \n",
    "                         header = False, \n",
    "                         inferSchema = True)\n",
    "\n",
    "iris_df.printSchema()\n",
    "\n",
    "setosa_udf = udf(lambda x: \"a\" if x == \"Iris-setosa\" else \"b\", StringType())\n",
    "iris_df = iris_df.withColumn(\"_c5\", setosa_udf(iris_df[\"_c4\"]))\n",
    "\n",
    "pred_vars = ['_c0', '_c1', '_c2', '_c3']\n",
    "\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols = pred_vars, \n",
    "    outputCol = 'features')\n",
    "vec_iris_df = vectorAssembler.transform(iris_df)\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"_c5\", outputCol=\"categoryIndex\")\n",
    "vec_iris_df = indexer.fit(vec_iris_df).transform(vec_iris_df)\n",
    "\n",
    "vec_iris_df.sample(False, .05).show()\n",
    "\n",
    "vec_iris_df = vec_iris_df.withColumn(\"label\", vec_iris_df[\"categoryIndex\"])\n",
    "\n",
    "train_iris_df, test_iris_df = vec_iris_df.randomSplit([0.9, 0.1], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_iris_df)\n",
    "\n",
    "#predict\n",
    "predictions = lrModel.transform(test_iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionAndLabels = predictions.rdd.map(lambda x: (x.prediction, x.label))\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "print(\"accuracy: {}\".format(metrics.accuracy))\n",
    "print(\"recall: {}\".format(metrics.recall()))\n",
    "print(\"precision: {}\".format(metrics.precision()))\n",
    "print(\"f1 measure: {}\".format(metrics.fMeasure()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porductionalization\n",
    "\n",
    "When dealing with productionalization, we can divide the way our model will be behaving into 4 flavours:\n",
    "\n",
    "![alt text](img/prod_grid.png)\n",
    "\n",
    "The main tow things to take into account are:\n",
    "\n",
    "* How prediction will be running: ranging from more static to more dynamic, we can differentiate between running a model over a static set or letting our users to query the model as they need to.\n",
    "\n",
    "* How learning will be running: ranging from more static to more dynamic, we can differentiate between running the learning process once or re-learning as soon as we acquire new labeled samples.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
