{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this session is to introduce some of the basic data cleaning concepts and identify which are the sources of problems from gathering to transformations.\n",
    "\n",
    "It is often said that 80% of data analysis is spent on the process of cleaning and preparing the data (Dasu T, Johnson T (2003). Exploratory Data Mining and Data Cleaning. Wiley-IEEE.), so it's worth devoting some time to understand the main issues we can find in this cleaning process.\n",
    "\n",
    "The main problem Data Cleaning tries to solve is to transform Dirty Data into data that is ready for analysis or any other kind of meaningful management (i.e. model building, summarization, visualization)\n",
    "\n",
    "### What is Dirty Data?\n",
    "There's probably no definition of dirty data nor clean data, so we will try to answer this question by identifying data sources and the problems they have when we try to process the data.\n",
    "\n",
    "### What you will learn in this session\n",
    "* Which are the main sources of data\n",
    "* What we understand by Dirty Data and Clean Data\n",
    "* How to apporach the Data Analysis problem in 5 steps\n",
    "* What kind of problems we may face during the process\n",
    "* What kind of data can a data source contain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "* [Data Formats and Types](#Data-Formats-and-Types)\n",
    "    * [Data Set (dataset)](#Data-Set-(dataset))\n",
    "    * [Structured Data vs. Unstructured Data](#Structured-Data-vs.-Unstructured-Data)\n",
    "    * [High-dimensional Data](#High-dimensional-data)\n",
    "    * [Descriptive Data](#Descriptive-data)\n",
    "    * [Longitudinal Data](#Longitudinal-data)\n",
    "    * [File Types](#File-Types)\n",
    "    * [Databases](#Databases)\n",
    "* [Data Sources](#Data-Sources)\n",
    "    * [Federated Data](#Federated-data)\n",
    "    * [Streaming Data](#Streaming-data)\n",
    "    * [Web (scrapped) Data](#Web-(scraped)-data)\n",
    "    * [API Data](#API-data)\n",
    "* [Data Quality](#Data-Quality)\n",
    "    * [Data Quality Properties](#Data-Quality-Properties)\n",
    "    * [Data Quality Problems (where can Dirty Data arise?)](#Data-Quality-Problems-(where-can-Dirty-Data-arise?))\n",
    "* [Data Anlysis in five steps](#Data-Analysis-in-Five-Steps)\n",
    "* [Exercises](#Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formats and Types\n",
    "\n",
    "### Data Set (dataset)\n",
    "\n",
    "A data set (or dataset) is a collection of data. \n",
    "\n",
    "We can see datasets as 'tables' with and arbitrary number of rows ($n$) and columns ($p$). Each row is considered an obervation and each column is considered a variable.\n",
    "\n",
    "\n",
    "* **Variables:** For a computer programmer, a variable is a memory space that can be filled with a know (or unknown) quantity of information (a.k.a. value). Moreover, this space has an associated notation alias that can be used in a program in running time to modify the value of the variable. Well, don't take this as an exact definition, but it helps to provide us a general refresh of what a variable is (for us the CS).\n",
    "\n",
    "    Well, statisticians have their own variables, lets have an (again) informal definition. In statistics, a variable is an attribute that describes a person, place, thing, or idea (often referred as feature). \n",
    "\n",
    "    As an example, we can take the list of physical characteristics of 10 persons. The objects of the matrix are the persons, the variables are the measured properties, such as the weight or the color of the eyes. \n",
    "\n",
    "    Variables can be classified as **qualitative** (*aka, categorical*) or **quantitative** (*aka, numeric*).\n",
    "\n",
    "    * **Qualitative:** Qualitative variables take on values that are **names** or **labels**. The eye color (e.g., brown, green, gray, etc.) or the sex of the person (e.g., female, male) would be examples of qualitative or categorical variables.\n",
    "\n",
    "    * **Quantitative:** Quantitative variables are numeric. They represent a measurable quantity. For example, when we speak of the age of a person, we are talking about the time past from its birth - a measurable attribute of the persom. Therefore, age would be a qualitative variable.\n",
    "\n",
    "\n",
    "* **From a DB point of view:** In the case of tabular data, a data set corresponds to one or more database tables, where every column of a table represents a particular variable, and each row corresponds to a given record of the data set in question. \n",
    "\n",
    "    The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set. Each value is known as a **datum**. Data sets can also consist of a collection of documents or files. \n",
    "\n",
    "    Several characteristics define a data set's structure and properties. These include the number and types of the attributes or variables.\n",
    "\n",
    "\n",
    "* **Special values**: all data in the same column (variable) belongs to the same type, however we can find missing values, meaning that this specific value is not present in the dataset\n",
    "\n",
    "### Structured Data vs. Unstructured Data\n",
    "\n",
    "\n",
    "* **Structured Data** (or structured information) is information that has a pre-defined data model or is organized in a pre-defined manner.\n",
    "\n",
    "    We understand as  data model  an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. \n",
    "\n",
    "    For instance, structured data usually resides in relational databases (RDBMS) which organizes data in tables with fields.  \n",
    "\n",
    "* **Unstructured Data** has no explicit model or schema (but the may have an internal model). It may be textual (for instance a book) or non-textual (for instance an image)\n",
    "\n",
    "\n",
    "* **Semi-structured Data** (https://en.wikipedia.org/wiki/Semi-structured_data) is a form of structured data that does not obey the formal structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            _              |                                                                      **Structured Data**                                                                     |                                          **Unstructured Data**                                          |\n",
    "|:------------------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------:|\n",
    "|    **Characteristics**   |  * Predefined data models * Usually text only * Easy to search                                                                                               |  * No predefined data model  * May be text, images, sound, video or other formats * Difficult to search |\n",
    "|      **Resides in**      |  * Relational databases * Data Warehouses                                                                                                                    |  * Applications * NoSQL databases * Data warehouses * Data lakes                                        |\n",
    "|     **Generated by**     |                                                                      Humans or machines                                                                      |                                            Humans or machines                                           |\n",
    "| **Typical applications** |  * Airline reservation systems * Inventory control * CRM systems * ERP systems                                                                               |  * Word processing * Email clients * Media processing tools                                             |\n",
    "|       **Examples**       |  * Dates * Phone numbers * Social Security numbers * Credit card numbers * Customer names * Addresses * Product names and numbers * Transactions information |  * Text files * Reports * Email messages * Audio files * Video files * Images * Surveillance imagery    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High dimensional data\n",
    "Let’s say we have $n$ samples (a.k.a. data points, instances) and $p$ features (a.k.a. attributes, independent variables, explanatory variables). \n",
    "\n",
    "There's no precise definition of how many dimensions are \"high dimensional\" so the imprecise definition is that high dimensional data is simply a data set with a very large p (sometimes around 1000)\n",
    "\n",
    "* **Images:** for example a 320x480 image means that we have 153600 variables which exeeds by a lot these $p$ limit. Note that 320x480 is for a single image ($n$ = 1)\n",
    "* **Videos:** with videos we just add a new dimension to our data: time. Videos are a sequence of images indexed by time. We have multiple pictures by second, so even a short video with low resolution can contain milions of variables.\n",
    "* **Categorical data:** Sometimes we want to handle categorical data as numerical data. One of the most straightforward ways of doing so is using the one-hot encoder, where for each possible value of the categorical variable, we create a new boolean variable which represents the presence or absence for a certain value of a certain cariable for each sample. This may lead to a high dimensional dataset. \n",
    "    For example if we have 20 categorical variables with each 50 posible values, by using a one-hot enncoding approach we will have to create a new dataset with 1000 numerical variables.\n",
    "* **Natural Language:** we can also have high-dimensional data in Natural Language when we deal with document representation. If we take a Bag of Words approach, where we represent each document/sentence as set of the frequency of its words we have to assign a value to each word for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Sparsity and Data Density\n",
    "\n",
    "We may have scenarios with high dimensional spaces that are dense (i.e. images, videos), this means that all variables have meaningful information.\n",
    "\n",
    "For example, if we load a picture, we'll see that all variables have values representing the colors in a certain area of the picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Grace_Hopper.jpg/256px-Grace_Hopper.jpg\"\n",
    "r = requests.get(url)\n",
    "\n",
    "if r.status_code == 200:\n",
    "    img_content = r.content\n",
    "    img = Image.open(io.BytesIO(img_content))\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape this image to a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(img).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check how many values are zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(np.array(img).reshape(-1) == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"This is a {}% of the variables\".format(\n",
    "    ((np.array(img).reshape(-1) == 0).sum()/np.array(img).reshape(-1).shape[0])*100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as a comparative, let's encode 3 sentences using a Bag of Words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence1 = \"Grace Brewster Murray Hopper (née Murray December 9, 1906 – January 1, 1992) \" + \\\n",
    "             \"was an American computer scientist and United States Navy rear admiral.\"\n",
    "sentence2 = \"One of the first programmers of the Harvard Mark I computer, she was a pioneer\" + \\\n",
    "             \"of computer programming who invented one of the first linkers.\"\n",
    "sentence3 = \"She popularized the idea of machine-independent programming languages, which led\" + \\\n",
    "             \"to the development of COBOL, an early high-level programming language still in use today. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    rules = [\n",
    "        {r\"([^\\w\\s])(\\w|\\d)\": r\"\\1 \\2\"},  # spaces symbols\n",
    "        {r\"(\\w|\\d)([^\\w\\s])\": r\"\\1 \\2\"},  # spaces symbols\n",
    "        {r\"\\s+\": u\" \"},  # multiple spaces to one\n",
    "    ]\n",
    "    for rule in rules:\n",
    "        for (k, v) in rule.items():\n",
    "            regex = re.compile(k)\n",
    "            text = regex.sub(v, text)\n",
    "    text = text.rstrip()\n",
    "    return text.lower()\n",
    "\n",
    "word_set = set()\n",
    "for s in [sentence1, sentence2, sentence3]:\n",
    "    clean_sentence = text_cleaner(s)\n",
    "    word_set.update(clean_sentence.split(\" \"))\n",
    "    \n",
    "print(\"We have {} different words\".format(len(word_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_as_bow = []\n",
    "for s in [sentence1, sentence2, sentence3]:\n",
    "    clean_sentence = text_cleaner(s)\n",
    "    bow = {w:0 for w in word_set}\n",
    "    for word in clean_sentence.split(\" \"):\n",
    "        if word in bow.keys():\n",
    "            bow[word] += 1\n",
    "    \n",
    "    sentences_as_bow.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_zeros = 0\n",
    "for sentence in sentences_as_bow:\n",
    "    for value in sentence.values():\n",
    "        if value == 0:\n",
    "            n_zeros += 1\n",
    "print(\"Number of zeros: {}\".format(n_zeros))\n",
    "print(\"This is a {}% of the variables\".format(n_zeros/(len(word_set)*len(sentences_as_bow))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.array([v for v in sentences_as_bow[0].values()]).reshape(1,-1), aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Course of dimensionality (from [KDnuggets post](https://www.kdnuggets.com/2017/04/must-know-curse-dimensionality.html))\n",
    "\n",
    "*\"As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially.\"*\n",
    "\n",
    "- Charles Isbell, Professor and Senior Associate Dean, School of Interactive Computing, Georgia Tech\n",
    "\n",
    "So imagine we have 3 observations of an individuum (*i.e.* age of a cow) that is ranged from 0 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [3, 2, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly small dataset, however our dimension space is 10 as we may want to represent any obsevation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# size in bytes\n",
    "x_space = np.linspace(0,10)\n",
    "print(\"%d bytes\" % (x_space.nbytes))\n",
    "print(\"space dimension: {}\".format(max(x_space)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to add a new feature (*i.e.* colour). This variable has a range of 10 colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [0, 1, 2]\n",
    "y_space = np.linspace(0,10)\n",
    "\n",
    "# size in bytes\n",
    "print(\"%d bytes\" % (np.array([x_space, y_space]).nbytes))\n",
    "print(\"space dimension: {}\".format(max(x_space) * max(y_space)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we add a new variable. Say number of diseases during first year, and we again range this variable to 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = [0, 6, 9]\n",
    "z_space = np.linspace(0,10)\n",
    "\n",
    "# size in bytes\n",
    "print(\"%d bytes\" % (np.array([x_space, y_space, z_space]).nbytes))\n",
    "print(\"space dimension: {}\".format(max(x_space) * max(y_space) * max(z_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(x,y,z, s=60)\n",
    "ax.set_xlim([0, 10])\n",
    "ax.set_xlabel('age (years)')\n",
    "\n",
    "ax.set_ylim([0, 10])\n",
    "ax.set_ylabel('colour')\n",
    "\n",
    "ax.set_zlim([0, 10])\n",
    "ax.set_zlabel('diseases')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigger than memory problem\n",
    "\n",
    "This problem arises when our dataset is bigger than our RAM memory, maing the dataset untractable atomically.\n",
    "\n",
    "It is a common problem when dealing with high-dimensional data (*i.e.* images, videos).\n",
    "\n",
    "Take into account that we may have a large Hard Disk, where our data set can be allocated without any problem, but as if we want to perform any operation to the whole dataset, we will have to load it into the RAM memory.\n",
    "\n",
    "The most naïve way of doing such a task is splitting the dataset, and processing it in batches. So, the most reasonable way is to define a batch size, load the data as small batches and perform the oprtations to these batche befor saving it again to the stoarge media.\n",
    "\n",
    "Another alternative is using a BigData framework, such as Spark or Dask, where data is loaded in a lazy manner and distributed in a computing cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive data\n",
    "A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features of a collection of information. \n",
    "\n",
    "So, we will say that we have a descriptive dataset when we find summary statistics of subjects.\n",
    "\n",
    "For example: https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736176906&menu=resultados&idp=1254735576820\n",
    "\n",
    "This kind of data is very useful for filling and enriching other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longitudinal data\n",
    "\n",
    "Longitudinal data, sometimes called panel data, is a collection of repeated observations of the same subjects, taken from a larger population, over some time  (track the same sample at different points in time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Types\n",
    "* **csv (Comma Separated Values):**  it is a type of document for easy access in tabular format, where columns are separated using comas and rows are separated using new space symbols.\n",
    "```csv\n",
    "name, age, sex\n",
    "alice, 34, female\n",
    "bob, 43, male\n",
    "```\n",
    "* **json (JavaScript Object Notation):**  it is a text based open stanard designed for transfering human readeable data. It's derived from scripting languange JavaScript, as it is used for representing simple data structures and linked lists.\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"name\": \"alice\", \n",
    "        \"age\": 34, \n",
    "        \"sex\": \"female\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"bob\", \n",
    "        \"age\": 43, \n",
    "        \"sex\": \"male\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "* **pickle:** (*from: https://docs.python.org/3/library/pickle.html*) The pickle module implements binary protocols for serializing and de-serializing a Python object structure. \n",
    "    * *Pickling* is the process whereby a Python object hierarchy is converted into a byte stream, and \n",
    "    * *unpickling* is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy. \n",
    "    Pickling (and unpickling) is alternatively known as “serialization”, “marshalling,” 1 or “flattening”; however, to avoid confusion, the terms used here are “pickling” and “unpickling”.\n",
    "* **parquet:** (*from: https://en.wikipedia.org/wiki/Apache_Parquet*) Apache Parquet is a free and open-source column-oriented data storage format of the Apache Hadoop ecosystem.\n",
    "\n",
    "    Apache Parquet is implemented using the record-shredding and assembly algorithm, which accommodates the complex data structures that can be used to store the data. The values in each column are physically stored in contiguous memory locations and this columnar storage provides the following benefits:\n",
    "\n",
    "    * Column-wise compression is efficient and saves storage space\n",
    "    * Compression techniques specific to a type can be applied as the column values tend to be of the same type\n",
    "    * Queries that fetch specific column values need not read the entire row data thus improving performance\n",
    "    * Different encoding techniques can be applied to different columns\n",
    "\n",
    "### Databases\n",
    "#### SQL (Structured Query Language)\n",
    "It is a domain-specific language used in programming and designed for managing data held in a Relational Database Management System (RDBMS). It is particularly useful in handling structured data, i.e. data incorporating relations among entities and variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to load data from a database?:\n",
    "* **Option 1:** Dumping data into a file, then scrap (SQL example). We will use the example MySQL DB form: https://github.com/datacharmer/test_db. Clone the repo to `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/test_db/load_employees.dump\", \"r\") as f:\n",
    "    sql_dump = f.read()\n",
    "sql_dump[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_values = sql_dump.split(\"VALUES\")\n",
    "sql_values = [line.split(\",\")[:-1] for line in sql_values[1].split(\"\\n\")]\n",
    "sql_values[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_symbols(in_string):\n",
    "    out_string =  re.sub(r'[^\\w]', '', in_string)\n",
    "    return out_string\n",
    "\n",
    "clean_dataset = []\n",
    "for register in sql_values:\n",
    "    clean_dataset.append([clean_symbols(line) for line in register])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Option 2:** Progamatically quering (you'll need to install `mysql`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# put your credentials into a json file in resources/mysql_credentials.json\n",
    "def load_credentials(my_credentials):\n",
    "    with open(my_credentials, \"r\") as f:\n",
    "        cred = json.load(f)\n",
    "        \n",
    "    return cred[\"user\"], cred[\"password\"]\n",
    "\n",
    "user, password = load_credentials(\"../resources/mysql_credentials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import MySQLdb\n",
    "\n",
    "db = MySQLdb.connect(host=\"localhost\",    # your host, usually localhost\n",
    "                     user=user,         # your username\n",
    "                     passwd=password,  # your password\n",
    "                     db=\"employees\")        # name of the data base\n",
    "\n",
    "# you must create a Cursor object. It will let\n",
    "# you execute all the queries you need\n",
    "cur = db.cursor()\n",
    "\n",
    "# Use all the SQL you like\n",
    "cur.execute(\"SELECT * FROM employees\")\n",
    "\n",
    "employees = [row for row in cur.fetchall()]\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "employees[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "### Federated data\n",
    "\n",
    "Data coming from one data source or a group of data soruces through a unified model.\n",
    "\n",
    "### Streaming data\n",
    "Streaming data is data that is continuously generated by different sources\n",
    "* A increasing scenario due to IoT raise\n",
    "* High frequency: needs of high performance computing\n",
    "* High value degradation: validity of data is really short, it has to be consumed at high paces\n",
    "\n",
    "### Web (scraped) data\n",
    "\n",
    "Potentially unstructred data coded for visualization (i.e. HTML coding).\n",
    "\n",
    "It's very time consuming having to extract data, as the programmer has to understand the structure of the source code and selecting which fields are valuabe for scrapping.\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://www.infoelectoral.mir.es/infoelectoral/min/areaDescarga.html?method=inicio\"\n",
    "r = requests.get(url)\n",
    "\n",
    "if r.status_code == 200:\n",
    "    web_content = r.content\n",
    "    \n",
    "print(web_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have libraries, such as `BeautifulSoup` that help in the process of parsing HTML and XML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(web_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links_to_download = []\n",
    "for table in soup.find_all(\"table\"):\n",
    "    for link in table.find_all('a'):\n",
    "        links_to_download.append(link[\"href\"])\n",
    "        \n",
    "links_to_download[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API data\n",
    "\n",
    "API stands for Application Programming Interface and it is sometimes used as an endpoint to query data sources. There are several protocols, being REST the most used and extended nowadays.\n",
    "\n",
    "The advantage of this kind of data access is that is easy to understand, easy to query and most of the times data is well organized with high quality.\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://www.thecocktaildb.com/api/json/v1/1/search.php?s=margarita\"\n",
    "\n",
    "r = requests.get(url)\n",
    "if r.status_code == 200:\n",
    "    response = r.json()\n",
    "else:\n",
    "    response = \"Error querinf the API\"\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for drink in response['drinks']:\n",
    "    print(drink['strDrink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality \n",
    "\n",
    "### Data Quality Properties\n",
    "\n",
    "* **Accuracy**\n",
    "    * The data was recorded correctly (i.e. is accurated vs. the real measurements)\n",
    "* **Completeness**\n",
    "    * All relevant data was recorded\n",
    "* **Uniqueness**\n",
    "    * Entities are recorded once\n",
    "* **Timeliness**\n",
    "    * The data is kept up to date\n",
    "* **Consistency**\n",
    "    * The data agrees with itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Problems (where can Dirty Data arise?)\n",
    "##### Data is dirty on its own at its source\n",
    "\n",
    "* Parsing text into fields (separator issues)\n",
    "* Naming conventions: NYC vs New York\n",
    "* Different representations (2 vs Two)\n",
    "* Formatting issues – especially dates\n",
    "    \n",
    "#### During extraction, transformation or loading\n",
    "\n",
    "* Transformations corrupt the data (complexity of software pipelines)\n",
    "    * Fields too long (get truncated)\n",
    "* Data sets are clean but integration (i.e., combining them) screws them up\n",
    "    * Missing required field (e.g. key field)\n",
    "    * Primary key violation (from un- to structured or during integration\n",
    "    * Redundant Records (exact match or other)\n",
    "* 'Rare' errors can become frequent after transformation or integration\n",
    "    * Transformation/aggregation can make errors propagate\n",
    "* Data sets are clean but suffer “bit rot”\n",
    "    * Data degradation is the gradual corruption of computer data due to an accumulation of non-critical failures in a data storage\n",
    "* Old data loses its value/accuracy over time\n",
    "* Any combination of the above\n",
    "\n",
    "#### During analysis/modeling steps\n",
    "\n",
    "* Outliers\n",
    "* Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis in Five Steps\n",
    "\n",
    "Statistical data analysis can be divided into five steps:\n",
    "\n",
    "#### Raw Data\n",
    "Depends on the source, we can find already curated datasets with high quality. However, if we collect data ourseves or we recieve the data after extraction, transform and load we can find many errors.\n",
    "\n",
    "Among the errors we can find there are: wrong types, different variable encoding, data without labels, etc.\n",
    "\n",
    "#### Technically Correct Data\n",
    "At this stage, we transform raw data, checking types, normalizing. After this stage we want data to have the proper \"shape\" with correct names, types, labels. However variables may be out of range or potentially inconsistent (relations between variables)  \n",
    "\n",
    "#### Consistent Data\n",
    "At this stage we analyze technically correct data using expert knoweldge. Data has to hace coherence among all its records. After this stage data is ready for statistical inference. For example the total amount of incomes in a year is the sum of all months incomes.\n",
    "\n",
    "#### Statistical Results\n",
    "At this stage the data cleaning process has finished and we can perform analysis, build models or extract insights\n",
    ":** analyze, build models, extract isnights\n",
    "\n",
    "#### Formatted Output\n",
    "\n",
    "At this stage, processed data is outputed in a tabular manner to external data storages or ploted in reports or dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the file [`iqsize.csv`](data/iqsize.csv) using the Python Standard library. Te result of this loading process must be a list containing list of data rows. You can use the `csv` library if you want (it is not mandatory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Once you have loaded `iqsize.csv` access and return the 3rd value in 2nd column. Do you think is there any advantage of using dictionaries instead of lists?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load `iqsize.csv`, but now you have to use dictionaries. What are the problems you have faced? Has the access to your dataset data improved in some way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Identify dataset variables. Are they quantitative or qualitative? Can you identify the units? If you have enough time change unit to metric system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Check the range of quantitative variables. Do you think the values are correct? If the values are not correct how would you correct it (don't waste time programming it)? (*hint:* if you have and error, treat the exception and pass it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Check the labels of qualitative variables. Do you think the values are correct? If the values are not correct them, how would you correct them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Compute the mean and the median of quantitative variables in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. For qualitative variables, count the frequency of each label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Repeat exercise 7, but now compute the mean and the median of each sex."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
