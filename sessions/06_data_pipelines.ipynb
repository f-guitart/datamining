{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipelines\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We have learnt that data is not always in the ideal format, and thus, it has to be transformed in order to make it suitable for analysing or ingesting to learning algorithms.\n",
    "\n",
    "`scikit-learn` provides an unified way of integrating data transformations, so the whole process can be run in an homogenized way.\n",
    "\n",
    "## What you will learn in this session\n",
    "\n",
    "* Understand the Pipeline paradigm\n",
    "* Know how to create Pipelines using `scikit-learn`\n",
    "* How to convert Categoric variables\n",
    "* How to transform Numeric Variables\n",
    "\n",
    "## Contents\n",
    "* [What is a Pipeline?](#What-is-a-Pipeline?)\n",
    "* [What is `scikit-learn`](#What-is-scikit-learn?)\n",
    "    * [`scikit-learn` Transformers](#scikit-learn-Transformers)\n",
    "    * [`scikit-learn` Pipelines](#scikit-learn-Pipelines)\n",
    "* [Transforming Variables](#Transforming-Variables)\n",
    "    * [Categorical to Numeric](#Categorical-to-Numeric)\n",
    "    * [Numeric Data](#Numeric-Data)\n",
    "    * [Heterogeneous Data](#Heterogeneous-Data)\n",
    "\n",
    "## Acknowledgments\n",
    "* https://pbpython.com/categorical-encoding.html\n",
    "* https://ramhiser.com/post/2018-04-16-building-scikit-learn-pipeline-with-pandas-dataframe/\n",
    "* https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n",
    "* https://chrisalbon.com/python/data_wrangling/pandas_create_pipeline/\n",
    "* https://scikit-learn.org/stable/modules/compose.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Pipeline?\n",
    "\n",
    "We have seen that preparing data for analysis may involve several steps, including different functions for data transformation.\n",
    "\n",
    "When we have completed the design of a data preparation, often we want to apply same transformations to different data sets. \n",
    "\n",
    "For example, we have an update of the previous dataset, and we want to apply same previous transformations (one after the following one) to new data.\n",
    "\n",
    "The process of chaining a series of transformations over a dataset is commonly know as Pipeline.\n",
    "\n",
    "Let's see a basic example:\n",
    "\n",
    "* We have designed a 3 step process\n",
    "    * select columns\n",
    "    * convert strings\n",
    "    * replace `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(in_df, column_list):\n",
    "    return in_df.loc[:, column_list]\n",
    "\n",
    "def name_lower(in_df, column):\n",
    "    in_df[column] = in_df.loc[:, column].str.lower()\n",
    "    return in_df\n",
    "\n",
    "def replace_nan(in_df, column, replace_value):\n",
    "    in_df[column] = in_df[column].fillna(replace_value) \n",
    "    return in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"age\": [12, 42, 24, np.nan],\n",
    "    \"name\": [\"alice\", \"BOB\", \"Charlie\", \"dan\"],\n",
    "    \"city\": [\"Lleida\", \"Barcelona\", \"New York\", \"Dubai\"]\n",
    "}\n",
    ")\n",
    "\n",
    "out_df = select_columns(df, [\"age\", \"name\"])\n",
    "out_df = name_lower(out_df, \"name\")\n",
    "mean_age = out_df[\"age\"].mean()\n",
    "out_df = replace_nan(out_df, \"age\", mean_age)\n",
    "out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `pandas.DataFrame.pipe`\n",
    "\n",
    "We can use `DataFrame.pipe` when chaining together functions that expect `Series`, `DataFrames` or `GroupBy` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.pipe(select_columns, column_list=[\"age\", \"name\"])\n",
    "    .pipe(name_lower, column=\"name\")\n",
    "    .pipe(replace_nan, column=\"age\", replace_value=out_df[\"age\"].mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `scikit-learn`?\n",
    "\n",
    "`scikit-learn` is a Machine Learning library written in Python. \n",
    "\n",
    "It contains most of the state-of-the-art algorithms such as KNN, XGBoost, random forest, SVM among others. \n",
    "\n",
    "It also contains several tools for data management.\n",
    "\n",
    "It's built upon some of the technology have already seen, like `NumPy`, `pandas`, and `Matplotlib`!\n",
    "\n",
    "It is also widely known for having an API that has become a kind of standard in the Machine Learning community. As a very fast summary, it contains two types of algorithms:\n",
    "\n",
    "* **Transformers:** these have two main methods: `.fit()` and `.transform()`\n",
    "* **Estimators:** these have two main methods: `.fit()` and `.predict()`\n",
    "\n",
    "Normally estimators are algorithms that return models.\n",
    "\n",
    "On the other hand, transformers are data management algorithms, that take as input `numpy.arrays` and return `numpy.arrays`.\n",
    "\n",
    "Using this API, one can chain transformers and estimators to define a Pipeline.\n",
    "\n",
    "### `scikit-learn` Transformers\n",
    "\n",
    "A Transformer is an object that has two main methods: `.fit(X, y=None)` and `.transform(X)` \n",
    "* `.fit(X, y=None)` takes `X` values and sets up a mapping between `X` values and values in a new domain\n",
    "* `transform(X)` takes `X` values and maps them to values in the new domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DumbFeaturizer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if len(X)%2 == 0:\n",
    "            self.middle = X[(len(X)//2)-1:(len(X)//2)+1]/2\n",
    "        else:\n",
    "            self.middle = X[(len(X)//2)]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [0 if x > self.middle else 1 for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb = DumbFeaturizer()\n",
    "dumb.fit([1,2,3,4,5,6,7])\n",
    "dumb.transform([-1,2,3,6, 7, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scikit-learn` Pipelines\n",
    "\n",
    "Calling fit on the pipeline is the same as calling fit on each estimator in turn, transform the input and pass it on to the next step. \n",
    "\n",
    "The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. \n",
    "\n",
    "If the last estimator is a transformer, again, so is the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [(\"dumb_feat1\", DumbFeaturizer()), (\"dumb_feat2\", DumbFeaturizer())]\n",
    "p = Pipeline(steps)\n",
    "\n",
    "p.fit_transform([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Variables\n",
    "\n",
    "We will see how to use `Transformers` and `Pipelines` to convert variables in order to be able to train models in `scikit-learn`.\n",
    "\n",
    "To this end we will use a real dataset: automobile dataset.\n",
    "\n",
    "### Automobile Dataset\n",
    "\n",
    "This data set consists of three types of entities: \n",
    "1. the specification of an auto in terms of various characteristics\n",
    "2. its assigned insurance risk rating\n",
    "3. its normalized losses in use as compared to other cars\n",
    "\n",
    "For more info: https://archive.ics.uci.edu/ml/datasets/Automobile\n",
    "\n",
    "It is an interesting dataset because it has a mix of categoric and numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "headers = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\n",
    "           \"num_doors\", \"body_style\", \"drive_wheels\", \"engine_location\",\n",
    "           \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n",
    "           \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\",\n",
    "           \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\",\n",
    "           \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"http://mlr.cs.umass.edu/ml/machine-learning-databases/autos/imports-85.data\",\n",
    "     header=None, \n",
    "    names=headers, \n",
    "    na_values=\"?\"\n",
    ")\n",
    "\n",
    "df.sample().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `doors` and `num_cylinders` are encoded as strings, while they seem numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.num_doors.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical to Numeric\n",
    "\n",
    "Some algorithms can not handle categorical data, so a conversion to numerical has to be done before using any `Estimator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `sklearn.preprocessing.LabelEncoder`\n",
    "\n",
    "Encode labels with value between 0 and `n_classes-1`. \n",
    "\n",
    "This is specially useful for encoding classification target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# we have to all the values in the variable range\n",
    "_ = le.fit([1, 2, 2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.transform([1, 1, 2, 6]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to transform a variable value not in the range fitted we get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.transform([3]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a transformed variable we can have the original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.inverse_transform([0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "_ = le.fit(df.make)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.transform(['mazda',\n",
    " 'mitsubishi',\n",
    " 'nissan',\n",
    " 'peugot',\n",
    " 'porsche',]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(le.inverse_transform([2, 2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `sklearn.preprocessing.OneHotEncoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems of transforming a categoric variable into a integer value is that our learning algorithm can use values as weights and provide biased results.\n",
    "\n",
    "To avoid this behaviour we can use the so called One Hot Encoding.\n",
    "\n",
    "The basic strategy is to convert each category value into a new column and assigns a 1 or 0 (True/False) value to the column. \n",
    "\n",
    "This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set.\n",
    "\n",
    "Let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.transform([['Female', 1], ['Male', 4]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the sparsity it provokes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit([[m] for m in df.make.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.transform([[\"peugot\"]]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `sklearn.feature_extraction.text.CountVectorizer`\n",
    "\n",
    "We have worked with text, and using it as data set feature is not easy.\n",
    "\n",
    "One common approach it to count how many words from a corpus there is in each text. A corpus is a set of words.\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Data\n",
    "#### Standardization, or mean removal and variance scaling\n",
    "\n",
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. \n",
    "\n",
    "If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n",
    "\n",
    "The function scale provides a quick and easy way to perform this operation on a single array-like dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df.length.plot(kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "X_scaled = preprocessing.scale(df.length)\n",
    "\n",
    "pd.Series(X_scaled).plot(kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing module further provides a utility class `StandardScaler` that implements the `Transformer` API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. \n",
    "\n",
    "This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit([[l] for l in df.length.values])\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.scale_                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(X_train)                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler instance can then be used on new data to transform it the same way it did on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [v for l in scaler.transform([[l] for l in df.length.values]) for v in l]\n",
    "pd.Series(flat_list).plot(kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to disable either centering or scaling by either passing `with_mean=False` or `with_std=False` to the constructor of `StandardScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling features to a range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. \n",
    "\n",
    "This can be achieved using `MinMaxScaler` or `MaxAbsScaler`, respectively.\n",
    "\n",
    "The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.\n",
    "\n",
    "Here is an example to scale a toy data matrix to the `[0, 1]` range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.wheel_base.plot(kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.scale(df.wheel_base)\n",
    "pd.Series(X_scaled).plot(kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform([[l] for l in df.wheel_base.values])\n",
    "\n",
    "flat_list = [v for l in X_train_minmax for v in l]\n",
    "pd.Series(flat_list).plot(kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[-3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler.scale_                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler.min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `MinMaxScaler` is given an explicit `feature_range=(min, max)` the full formula is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.wheel_base.values\n",
    "x_max, x_min = X.max(), X.min()\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (x_max - x_min) + x_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MaxAbsScaler` works in a very similar fashion, but scales in a way that the training data lies within the range `[-1, 1]` by dividing through the largest maximum value in each feature. \n",
    "\n",
    "It is meant for data that is already centered at zero or sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform([[v] for v in df.wheel_base])\n",
    "\n",
    "flat_list = [v for l in X_train_maxabs for v in l]\n",
    "pd.Series(flat_list).plot(kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_scaler.scale_         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with scale, the module further provides convenience functions minmax_scale and maxabs_scale if you don't want to create an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous Data\n",
    "\n",
    "Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps. \n",
    "\n",
    "Often it is easiest to preprocess data before applying `scikit-learn` methods, for example using `pandas`. \n",
    "\n",
    "Processing your data before passing it to `scikit-learn` might be problematic for one of the following reasons:\n",
    "\n",
    "* Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as data leakage), for example in the case of scalers or imputing missing values.\n",
    "\n",
    "* You may want to include the parameters of the preprocessors in a parameter search.\n",
    "\n",
    "The `ColumnTransformer` helps performing different transformations for different columns of the data, within a `Pipeline` that is safe from data leakage and that can be parametrized. \n",
    "\n",
    "`ColumnTransformer` works on arrays, sparse matrices, and pandas `DataFrames`.\n",
    "\n",
    "To each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data, we might want to encode `make` column as a categorical variable using `preprocessing.OneHotEncoder` but apply a `preprocessing.MinMaxScaler()` to the `length` column. \n",
    "\n",
    "As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say `make_category` and `scaled_length`. By default, the remaining rating columns are ignored (`remainder='drop'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "column_trans = ColumnTransformer(\n",
    "     [('make_category', OneHotEncoder(dtype='int'),['make']),\n",
    "      ('scaled_length', preprocessing.MinMaxScaler(), ['length'])],\n",
    "     remainder='drop')\n",
    "\n",
    "column_trans.fit(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans.transform(df).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep the remaining rating columns by setting `remainder='passthrough'`. The values are appended to the end of the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = ColumnTransformer(\n",
    "     [('make_category', OneHotEncoder(dtype='int'),['make']),\n",
    "      ('scaled_length', preprocessing.MinMaxScaler(), ['length'])],\n",
    "     remainder='passthrough')\n",
    "\n",
    "column_trans.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `remainder` parameter can be set to an estimator to transform the remaining rating columns. \n",
    "\n",
    "The transformed values are appended to the end of the transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = ColumnTransformer(\n",
    "     [('make_category', OneHotEncoder(), ['make']),\n",
    "      ('length_scaled', preprocessing.MinMaxScaler(), [\"length\"])],\n",
    "     remainder=preprocessing.MaxAbsScaler())\n",
    "\n",
    "column_trans.fit_transform(df.loc[:, [\"wheel_base\", \"length\", \"width\", \"height\", \"make\"]]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_column_transformer` function is available to more easily create a `ColumnTransformer` object. \n",
    "\n",
    "Specifically, the names will be given automatically. The equivalent for the above example would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "column_trans = make_column_transformer(\n",
    "     (OneHotEncoder(), ['make']),\n",
    "     (preprocessing.MinMaxScaler(), [\"length\"]),\n",
    "     remainder=preprocessing.MaxAbsScaler())\n",
    "column_trans "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
